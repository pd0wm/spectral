%!TEX program = xelatex

\documentclass{memoir}
\input{../../includes/preamble.tex}
\addbibresource{../../includes/bibliography.bib}

\begin{document}
\chapter{Review of Basic Concepts}
\section{Preliminaries}
\subsection{Convex functions}
\begin{definition}
    Let $f:S\to \mathbb{R}$ where $S$ is a convex subset of $\mathbb{R}$. Then $f$ is a \textbf{convex function} if for all $x_1,x_2 \in S$ and $\lambda \in [0,1]$
    \begin{align*}
        \lambda f(x_1) + (1- \lambda)f(x_2) \ge f( \lambda x_1 + (1- \lambda) x_2).
    \end{align*}
\end{definition}

\begin{theorem}
    Let $f:S \to \mathbb{R}$ be a convex function and let $\lambda_1,\ldots,\lambda_n \in \mathbb{R}$ be such that
    \begin{align*}
        \lambda_1 + \lambda_2 + \ldots + \lambda_n = 1.
    \end{align*}
    Then for all $x_1,\ldots,x_n \in S$
    \begin{align*}
        \lambda_1 f(x_1) + \lambda_2 f(x_2) + \ldots + \lambda_n f(x_n) \ge f(\lambda_1 x_1 + \lambda_2 x_2 + \ldots + \lambda_n x_n).
    \end{align*}
\end{theorem}
\begin{proof}
    We will proof by induction. Since $f$ is convex, the base case holds. Suppose that the statement is true for a certain $n$. Then, since $f$ is convex
    \begin{align*}
        &\lambda_1 f(x_1) + \lambda_2 f(x_2) + \ldots + \lambda_{n+1} f(x_{n+1}) \\
        &= (\lambda_1+\lambda_2)\left[\frac{\lambda_1}{\lambda_1 + \lambda_2} f(x_1) + \frac{\lambda_2}{\lambda_1+\lambda_2}f(x_2)\right] + \ldots + \lambda_{n+1} f(x_{n+1})  \\
        &\ge (\lambda_1 + \lambda_2)f\left( \frac{\lambda_1}{\lambda_1 + \lambda_2} x_1 + \frac{\lambda_2}{\lambda_1+\lambda_2}x_2\right) + \ldots + \lambda_{n+1} f(x_{n+1}) \\
        &\ge f\left[(\lambda_1 + \lambda_2) \frac{\lambda_1}{\lambda_1 + \lambda_2} x_1 + (\lambda_1 + \lambda_2)\frac{\lambda_2}{\lambda_1+\lambda_2}x_2\ldots + \lambda_{n+1}\right] \\
        &= f(\lambda_1 x_1 + \lambda_2 x_2 + \ldots + \lambda_{n+1} x_{n+1}).
    \end{align*}
\end{proof}

\section{Stochastic Processes}
\subsection{Basic Definitions}
\begin{definition}
    A \textbf{random variable} consists of an experiment with a probability measure $P[.]$ defined on a sample space $S$ and a function that assigns a real number to each outcome in the sample space of the experiment.
\end{definition}

\begin{definition}
    Subsets of the sample space are called \textbf{events}.
\end{definition}

\begin{definition}
    The \textbf{probability mass function} (PMF) of the discrete random variable $X$ is
    \begin{align*}
        P_X(x) = P[X=x].
    \end{align*}
\end{definition}

\begin{definition}
    The \textbf{cumulative distribution function} (CDF) of a random variable $X$ is
    \begin{align*}
        F_X(x) = P[X \le x].
    \end{align*}
\end{definition}
The CDF of a derived random variable $Y=g(X)$ is
\begin{align*}
    F_{Y}(x) = P[Y \le x] = P[g(X) \le x] = P[X \le g^{-1}(x)].
\end{align*}

\begin{definition}
    The \textbf{expected value} of a random variable $X$ is
    \begin{align*}
        E[X]=\mu_X = \sum_{x \in S_X} x P_X(x).
    \end{align*}
\end{definition}

\begin{definition}
    The \textbf{variance} of a random variable $X$ is
    \begin{align*}
        \Var[X] = E[(X-\mu_X)^2].
    \end{align*}
\end{definition}
The variance of a random variable $X$ simplifies to
\begin{align*}
    \Var[X] = E[X^2] - E^2[X].
\end{align*}

\subsection{Basic Theorems}

\begin{theorem}[Jensen's Inequality]
    Let $f$ be a convex function and $X$ a random variable. Then
    \begin{align*}
        E[f(X)] \ge f(E[X]).
    \end{align*}
\end{theorem}
\begin{proof}
    Note that
    \begin{align*}
        \sum_{x \in S_X} P_X(x) = 1.
    \end{align*}
    Then, by the definition of the expected value and the convexity of $f$
    \begin{align*}
        E[f(X)] &= \sum_{x \in S_X} f(x) P_X(x) \\
        &\ge f\left[\sum_{x \in S_X} x P_X(x) \right] \\
        &=f(E[X]).
    \end{align*}
\end{proof}

\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $X$ and $Y$ be random variables. Then
    \begin{align*}
        (E[XY])^2 \le E[X^2]E[Y^2].
    \end{align*}
\end{theorem}
\begin{proof}
    Let $a \in \mathbb{R}$, then
    \begin{align*}
        E[(a X + Y)^2] = E[X^2]a^2  + 2E[XY]a + E[Y^2].
    \end{align*}
    Consider this as a function of $a$. Note that  $E[(aX+Y)^2]\ge0$. So the obtained polynomial will have no zero-crossings. Therefore
    \begin{align*}
        (2 E[XY])^2-4 E[X^2] E[Y^2] \le 0.
    \end{align*}
\end{proof}

\subsection{Dependence}

\begin{definition}
    The \textbf{conditional probability} of an event $A$ given event $B$ is
    \begin{align*}
        P[A|B] = \frac{P[A \cap B]}{P[B]}.
    \end{align*}
\end{definition}

\begin{definition}
    Events $A$ and $B$ are \textbf{independent} if
    \begin{align*}
        P[A \cap B]=P[A] P[B].
    \end{align*}
\end{definition}

\begin{theorem}
    $A$ and $B$ are two independent events if and only if
    \begin{align*}
        P[A | B] = P[A].
    \end{align*}
\end{theorem}
\begin{proof}
    By the definitions
    \begin{align*}
        P[A | B] = \frac{P[A \cap B]}{P[B]} = \frac{P[A]P[B]}{P[B]} = P[A].
    \end{align*}
\end{proof}

\begin{definition}
    The \textbf{covariance} of two random variables $X$ and $Y$ is
    \begin{align*}
        \Cov[X,Y]=E[(X- \mu_X)(Y- \mu_Y)].
    \end{align*}
\end{definition}

\begin{definition}
    The \textbf{correlation} of two random variables $X$ and $Y$ is
    \begin{align*}
        r_{X,Y} = E[XY].
    \end{align*}
\end{definition}
The covariance of two random variables $X$ and $Y$ simplifies to
\begin{align*}
    \Cov[X,Y] = r_{X,Y} - \mu_X \mu_Y.
\end{align*}
The variance of the addition of two random variables $X$ and $Y$ simplifies to
\begin{align*}
    \Var[X+Y] = \Var[X] + \Var[Y] + 2 \Cov[X,Y].
\end{align*}
Consider two random variables $X$ and $Y$. If $\Cov[X,Y]=0$, then $X$ and $Y$ are \textbf{uncorrelated}. Correlation is a measure of dependence. If $\Cov[X,Y]>0$, then $X>\mu_X$ implies that we expect $Y>\mu_Y$.

If $r_{X,Y}=0$, then $X$ and $Y$ are \textbf{orthogonal}. In the case of zero-mean variables, orthogonal variables are uncorrelated.

\begin{definition}
    The \textbf{correlation coefficient} of two random variables $X$ and $Y$ is
    \begin{align*}
        \rho_{X,Y} = \frac{\Cov[X,Y]}{\sqrt{\Var[X]\Var[Y]}}.
    \end{align*}
\end{definition}

\begin{theorem}
    $|\rho_{X,Y}|\le1$ holds.
\end{theorem}
\begin{proof}
    We will proof that $\rho_{X,Y}^2 \le 1$. To this end, by the Cauchy-Schwarz Inquality
    \begin{align*}
        \Cov[X,Y]^2 &= E[(X- \mu_X)(Y- \mu_Y)]^2 \\
        &\le E[(X-\mu_X)^2] E[(Y- \mu_Y)^2] \\
        &= \Var[X] \Var[Y]
    \end{align*}
    Therefore
    \begin{align*}
        \rho_{X,Y}^2 = \frac{\Cov[X,Y]^2}{\Var[X]\Var[Y]} \le 1.
    \end{align*}
\end{proof}

\subsection{Normal Distribution}
\subsubsection{Gauss' Derivation of the Univariate Normal Distribution}
In deriving the normal distribution, Gauss assumed the following:
\begin{enumerate}
    \item Small error are more likely than large errors.
    \item The likelihood of errors of magnitudes $\varepsilon$ and $-\varepsilon$ are equal.
    \item When a quantity is measured several times, the most likely value is given by the average of the measurements.
\end{enumerate}
Let $p$ be the true value of the measured quantity. Let $n$ independent observations yield measurements $M_1,\ldots,M_n$ and let the probability density function of the error by given by $\phi(x)$. Then the assumptions translate to the following:
\begin{enumerate}
    \item $\phi(x)$ has a maximum at $x=0$.
    \item $\phi(x)=\phi(-x)$.
    \item The maximum likelihood estimate of $p$ is given by
    \begin{align*}
        \hat{p} = \frac{M_1 + \ldots + M_n}{n} = \mu.
    \end{align*}
\end{enumerate}
Based on our conclusions, the joint probability density function is given by
\begin{align*}
    f(x) = \phi(M_1 - x)\cdots\phi(M_n-x).
\end{align*}
Therefore the maximum likelihood estimate yields
\begin{align*}
    \left.\frac{\partial f}{\partial p}\right|_{p=\hat{p}}=
    -\left[\frac{\phi'(M_1-\hat{p})}{\phi(M_1-\hat{p})} + \ldots + \frac{\phi'(M_n-\hat{p})}{\phi(M_n-\hat{p})}\right]f(\hat{p})=0.
\end{align*}
Remember that the measurements are arbitrary. In further exploring the structure of $\phi(x)$, assume that
\begin{align*}
    M_1 = M, \qquad M_2=\ldots=M_n=M-nN
\end{align*}
for a real $N$. Then
\begin{align*}
    \hat{p}=\frac{nM - (n-1)nN}{n} =M-(n-1)N.
\end{align*}
Now our maximum likelihood estimate yields
\begin{align*}
    \frac{\phi'[(n-1)N]}{\phi[(n-1)N]} + (n-1)\frac{\phi'(-N)}{\phi(-N)} = \frac{\phi'[(n-1)N]}{\phi[(n-1)N]} - (n-1)\frac{\phi'(N)}{\phi(N)}=0.
\end{align*}
Using the fact that this holds for any $N$ and $\phi(x)$ obtains a maximum at $x=0$, it can be shown that
\begin{align*}
    \frac{\phi'(x)}{\phi(x)} = -\frac{x}{ \sigma^2}
\end{align*}
for any real $\sigma$. Finally, solving this differential equation and normalising the solution yields that
\begin{align*}
    \phi(x)=\frac{1}{\sigma \sqrt{2 \pi}}\exp\left[-\frac{x^2}{2 \sigma^2}\right].
\end{align*}
If $\phi(x)$ is the error probability density function of a random variable $X$, then we denote $X \sim N(\mu,\sigma^2)$.

\subsubsection{The Multivariate Normal Distribution}
Suppose that $\vec{Z}$ is a $n$ vector of independent random variables each $N(0,1)$ distributed. Then the joint probability density function is given by
\begin{align*}
    f_Z(\vec{Z}) = \prod_i \frac{1}{\sqrt{2 \pi}} \exp\left[-\frac{z_i^2}{2}\right] = \frac{1}{(2 \pi)^{n/2}} \exp\left[-\frac{1}{2}\vec{Z}^T \vec{Z}\right].
\end{align*}
Let $\mat{\Sigma}$ be a $n \times n$ symmetric positive-definite matrix. $\mat{\Sigma}$ is called the \textbf{covariance matrix}. Since $\mat{\Sigma}$ is symmetric positive-definite, we can write
\begin{align*}
    \mat{\Sigma} = \mat{Q}^T \mat{\Lambda} \mat{Q} = (\mat{Q}^T \mat{\Lambda}^{1/2} \mat{Q}) (\mat{Q}^T \mat{\Lambda}^{1/2} \mat{Q}) = \mat{\Sigma}^{1/2} \mat{\Sigma}^{1/2}.
\end{align*}
Note that $(\mat{\Sigma}^{1/2})^T = \mat{\Sigma}^{1/2}$
Let $\vec{X}$ be a $n$ vector such that
\begin{align*}
    \vec{X} = \mat{\Sigma}^{1/2} \vec{Z} + \vec{\mu}.
\end{align*}
Thus $\mat{\Sigma}$ defines a dependency between the variables of $\vec{X}$. We require $\mat{\Sigma}$ to be positive definite analogous to the requirement that $\sigma^2$ is positive. We require $\mat{\Sigma}$ to be symmetric since we assume the dependencies between the variables of $\vec{X}$ to be commutative. The inverse transformation is given by
\begin{align*}
    \vec{Z} = T(\vec{X}) = \mat{\Sigma}^{-1/2}(\vec{X}-\vec{\mu}).
\end{align*}
with Jacobian $J_T = \mat{\Sigma}^{-1/2}$. We can now use the change-of-variables theorem to write down the joint probability density function of $\vec{X}$
\begin{align*}
    f_X(\vec{X}) &= f_Z(T(\vec{X})) |J_T| \\
    &= \frac{1}{(2 \pi)^{n/2}} \exp\left\{-\frac{1}{2}[\mat{\Sigma}^{-1/2}(\vec{X}-\vec{\mu})]^T [\mat{\Sigma}^{-1/2}(\vec{X}-\vec{\mu})]\right\} |\mat{\Sigma}^{-1/2}| \\
    &= \frac{1}{(2 \pi)^{n/2} |\mat{\Sigma}|^{1/2}} \exp\left[-\frac{1}{2}(\vec{X}-\vec{\mu})^T \mat{\Sigma}^{-1} (\vec{X}-\vec{\mu})\right].
\end{align*}
This probability density function is known as the multivariate normal distribution.

\subsection{Stochastic Processes}

\begin{definition}
    A \textbf{stochastic process} $X(t)$ consists of an experiment with a probability measure $P[.]$ defined on a sample space $S$ and a function that assigns a time function $x(t,s)$ to each outcome $s$ in the sample space of the experiment.
\end{definition}

Suppose that we observe a random process $X(t)$ at a particular time $t_1$. Then each observation yields a $x(t_1,s)$. Therefore $x(t_1,s)$ is the sample value of a random variable. We denote this random variable by $X(t_1)$.

\begin{definition}
    A stochastic process $X(t)$ is \textbf{independently identically distributed} if
    \begin{enumerate}
        \item $X(t_k)$ for all $k$ are independent random variables,
        \item $X(t_k)$ for all $k$ have identical probability distribution functions.
    \end{enumerate}
\end{definition}

\begin{definition}
    The \textbf{expected value} of a stochastic process $X(t)$ is the function
    \begin{align*}
        \mu_X(t) = E[X(t)].
    \end{align*}
\end{definition}

\begin{definition}
    The \textbf{autocovariance} function of a stochastic process $X(t)$ is
    \begin{align*}
        C_X(t,\tau)=\Cov[X(t),X(t+\tau)].
    \end{align*}
\end{definition}

\begin{definition}
    The \textbf{autocorrelation} function of a stochastic process $X(t)$ is
    \begin{align*}
        R_X(t,\tau)=E[X(t),X(t + \tau)].
    \end{align*}
\end{definition}
The autocovariance of a stochastic process simplifies to
\begin{align*}
    C_X(t,\tau) = R_X(t,\tau) - \mu_X(t) \mu_X(t + \tau).
\end{align*}

\begin{definition}
    A stochastic process $X(t)$ is \textbf{wide sense stationary} if for all $t$
    \begin{enumerate}
        \item $\mu_X(t) = \mu_X$,
        \item $R_X(t,\tau)  =R_X(\tau)$.
    \end{enumerate}
\end{definition}

\begin{theorem}
    For a wide sense stationary process $X(t)$ the following hold:
    \begin{enumerate}
        \item $R_X(0) \ge 0$,
        \item $R_X(\tau) = R_X(-\tau)$,
        \item $R_X(0) \ge |R_X(\tau)|$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    The first property follows from the definition. To prove the second property, let us evaluate the autocorrelation function at $\tilde{t} = t + \tau$. Then
    \begin{align*}
        R_X(\tau) &= R_X(t,\tau)\\
        &= E[X(t)X(t+\tau)]\\
        &= E[X(\tilde{t} - \tau) X(\tilde{t})]\\
        &= E[X(\tilde{t}) X(\tilde{t} - \tau)]\\
        &= R_X(\tilde{t}, - \tau)\\
        &= R_X(-\tau).
    \end{align*}
    To prove the third property, we make use of the correlation coefficient. Note that
    \begin{align*}
        \Var[X(t)] = E[X^2(t)] -E^2[X(t)] = R_X(0) - \mu_X^2.
    \end{align*}
    So $\Var[X(t)]$ is constant for all $t$. Therefore
    \begin{align*}
        C_X(t,\tau) &= \rho_{X(t),X(t+\tau)} \sqrt{\Var[X(t)]\Var[X(t+\tau)]}\\
        &\le \sqrt{\Var[X(t)]\Var[X(t+\tau)]}\\
        &= C_X(0).
    \end{align*}
    Then
    \begin{align*}
        |R_X(\tau)| = [ C_X(t,\tau) + \mu_X^2]^2 \le [C_X(0) + \mu_X^2]^2 = R_X(0).
    \end{align*}
\end{proof}

\begin{definition}
    The \textbf{average power} of a wide sense stationary process $X(t)$ is $R_X(0)$.
\end{definition}

\begin{definition}
    A stochastic process $X(t)$ is \textbf{mean-ergodic} if the time average estimate
    \begin{align*}
        \hat{\mu}_X = \frac{1}{T} \int_0^T X(t) dt
    \end{align*}
    has zero mean square error in the limit.
\end{definition}

\begin{theorem}
    If a stochastic process $X(t)$ has a autocovariance function $C_X(\tau)$ which is absolutely integrable, then $X(t)$ is mean-ergodic.
\end{theorem}
\begin{proof}
    First we note that
    \begin{align*}
        E[\hat{\mu}_X] = \frac{1}{T} E\left[\int_0^T X(t) dt\right] = \frac{1}{T} \int_0^T E[X(t)] dt= \mu_X.
    \end{align*}
    So $\mu_X$ is unbiased. Next, since $C_X(\tau)$ is absolutely integrable, we calculate
    \begin{align*}
        \Var[\hat{\mu}_X] &= E\left[\left(\frac{1}{T}\int_0^T (X(t) - \mu_X)^2 dt\right)^2\right] \\
        &= \frac{1}{T^2} \int_0^T \int_0^T E[(X(t)-\mu_X)(X(t')-\mu_X)]dt' dt \\
        &= \frac{1}{T^2} \int_0^T \int_0^T C_X(t' - t)dt' dt \\
        &\le \frac{K}{T}.
    \end{align*}
    for some real $K$. Therefore $\Var[\hat{\mu}_X] \to 0$ if $T \to \infty$.
\end{proof}


\subsection{Power Spectral Density}
Consider a realisation $x(t)$ of a stochastic process $X(t)$. Its average power is calculated by
\begin{align*}
    P &= \lim_{T \to \infty} \frac{1}{T} \int_{-T/2}^{T/2} x(t) dt \\
    &= \lim_{T \to \infty} \frac{1}{T} \int_{-\infty}^{\infty} x_T(t) dt
\end{align*}
We now define $X_T(f) = \F[x_T(t)]$. Then by Parseval's theorem
\begin{align*}
    P &= \lim_{T \to \infty} \frac{1}{T} \int_{-\infty}^{\infty} |X_T(f)|^2 df \\
    &= \int_{-\infty}^{\infty} \lim_{T \to \infty} \frac{|X_T(f)|^2}{T} df.
\end{align*}
This motivates the following definition.
\begin{definition}
    The \textbf{power spectral density} (PSD) of a deterministic waveform $x(t)$ is
    \begin{align*}
        \mathcal{P}(f) = \lim_{T \to \infty} \frac{|X_T(f)|^2}{T}.
    \end{align*}
\end{definition}

\begin{theorem}[Wiener-Khinchin]
    The power spectral density of a wide sense stationary stochastic process $X(t)$ is calculated by
    \begin{align*}
        \mathcal{P}(f) = \F[R_X(\tau)] = \int_{-\infty}^{\infty} R_X(\tau) \exp(-j2\pi f \tau) d \tau.
    \end{align*}
\end{theorem}

More to come.

\section{Linear Algebra}
\subsection{Orthogonality}
\begin{definition}
    An $n \times n$ matrix $\mat{Q}$ whose columns form an orthonormal set is called an \textbf{orthogonal} matrix.
\end{definition}

\begin{theorem}
    The columns of and $m \times n$ matrix $\mat{Q}$ form an orthonormal set if and only if $\mat{Q}^T \mat{Q}=I_n$.
\end{theorem}
\begin{proof}
    Let $\vec{q}_i$ denote the $i$'th column of $\mat{Q}$. Then
    \begin{align*}
        (\mat{Q}^T \mat{Q})_{ij} = \vec{q}_i \cdot \vec{q}_j.
    \end{align*}
    The equivalence follows.
\end{proof}

\begin{definition}
    A square matrix $\mat{A}$ is orthogonally diagonalizable if there exists an orthonogal matrix $\mat{Q}$ and a diagonal matrix $\mat{D}$ such that
    \begin{align*}
        \mat{Q}^T \mat{A} \mat{Q} = \mat{D}.
    \end{align*}
\end{definition}

\begin{theorem}[Spectral Theorem]
    Let $\mat{A}$ be an $n \times n$ real matrix. Then $\mat{A}$ is symmetric if and only if it is orthogonally diagonalizable.
\end{theorem}
\begin{proof}
    Assume that $\mat{A}$ is orthogonally diagonalizable. Then we can write
    \begin{align*}
        \mat{A} = \mat{Q} \mat{D} \mat{Q}^T.
    \end{align*}
    Therefore
    \begin{align*}
        \mat{A}^T = (\mat{Q} \mat{D} \mat{Q}^T)^T = \mat{Q} \mat{D}^T \mat{Q}^T = \mat{Q} \mat{D} \mat{Q}^T = \mat{A}.
    \end{align*}
    So $\mat{A}$ is symmetric. The other part of the proof is omitted.
\end{proof}

\begin{theorem}
    Let $\mat{A}$ be an real symmetrix matrix. Then its eigenvalues are real.
\end{theorem}
\begin{proof}
    Suppose $\lambda$ is an eigenvalue of $\mat{A}$ with corresponding eigenvector $\vec{v}$. Then $\mat{A} \vec{v} = \lambda \vec{v}$ and $\mat{A} \bar{\vec{v}} = \bar{\lambda} \bar{\vec{v}}$. Therefore
    \begin{align*}
        \lambda (\bar{\vec{v}}^T \vec{v}) = \bar{\vec{v}}^T (\lambda \vec{v}) = \bar{\vec{v}}^T (\mat{A} \vec{v}) = (\bar{\vec{v}}^T \mat{A}) \vec{v} = (\mat{A} \bar{\vec{v}})^T \vec{v} = \bar{\lambda} (\bar{\vec{v}}^T \vec{v}).
    \end{align*}
    So
    \begin{align*}
        (\lambda - \bar{\lambda})(\bar{\vec{v}}^T \vec{v}) = (\lambda - \bar{\lambda}) |\vec{v}|^2 = 0.
    \end{align*}
    This implies that $\lambda = \bar{\lambda}$ and $\lambda$ must therefore be real.
\end{proof}

\subsection{Quadratic Form}
\begin{definition}
    A \textbf{quadratic form} in $n$ variables is a function $f: \mathbb{R}^n \to \mathbb{R}$ of the form
    \begin{align*}
        f(\vec{x}) = \vec{x}^T \mat{A} \vec{x}
    \end{align*}
    where $\mat{A}$ is a symmetric $n \times n$ matrix.
\end{definition}

\begin{theorem}[Principal Axes Theorem]
    Let $f(\vec{x})=\vec{x}^T \mat{A} \vec{x}$ be a quadratic form. Then there exists a change of variables $\vec{x} = T(\vec{y})$ such that
    \begin{align*}
        f(\vec{x}) = \lambda_1 y_1^2 + \ldots + \lambda_n y_n^2
    \end{align*}
    where $\lambda_1, \ldots, \lambda_n$ denote the eigenvalues of $\mat{A}$.
\end{theorem}
\begin{proof}
    Since $\mat{A}$ is symmetric, we can write
    \begin{align*}
        \mat{A} = \mat{Q} \mat{D} \mat{Q}^T.
    \end{align*}
    Now let $\vec{y} = \mat{Q}^T \vec{x}$. Then $\vec{x} = \mat{Q} \vec{y}$. Therefore
    \begin{align*}
        f(\vec{x}) &= f(\mat{Q} \vec{y}) \\
        &= (\mat{Q} \vec{y})^T (\mat{Q} \mat{D} \mat{Q}^T) (\mat{Q} \vec{y}) \\
        &= \vec{y}^T (\mat{Q}^T \mat{Q}) \mat{D} (\mat{Q}^T \mat{Q}) \vec{y} \\
        &= \vec{y}^T \mat{D} \vec{y} \\
        &= \lambda_1 y_1^2 + \ldots + \lambda_n y_n^2.
    \end{align*}
\end{proof}

\begin{definition}
    A quadratic form $f(\vec{x}) = \vec{x}^T \mat{A} \vec{x}$ is \textbf{positive definite} if for all $\vec{x} \neq 0$ holds that $f(\vec{x}) > 0$. A symmetric matrix $\mat{A}$ is \textbf{positive definite} if the associated quatric form is so.
\end{definition}

\begin{theorem}
    The quadratic form $f(\vec{x})=\vec{x}^T \mat{A} \vec{x}$ is positive definite if and only if all of the eigenvalues of $\mat{A}$ are positive.
\end{theorem}
\begin{proof}
    The equivalence follows directly from the principal axes theorem.
\end{proof}

\subsection{Complex Vectors and Matrices}
\begin{definition}
    If $\mat{A}$ is a complex matrix, then the \textbf{conjugate transpose} of $\mat{A}$ is given by
    \begin{align*}
        \mat{A}^* = \bar{\mat{A}}^T.
    \end{align*}
\end{definition}
The following definition is the complex analogue of a symmetric matrix.
\begin{definition}
    If a square matrix $\mat{A}$ is such that $\mat{A}^* = \mat{A}$, then $\mat{A}$ is called \textbf{Hermitian}.
\end{definition}
The following definition is the complex analogue of an orthogonal matrix.
\begin{definition}
    If a square matrix $\mat{A}$ is such that $\mat{A}^{-1} = \mat{A}^*$, then $\mat{A}$ is called \textbf{unitary}.
\end{definition}
\begin{definition}
    If a square matrix $\mat{A}$ is such that $\mat{A}^* \mat{A} = \mat{A} \mat{A}^*$, then $\mat{A}$ is called \textbf{normal}.
\end{definition}
\begin{theorem}
    If a square matrix $\mat{A}$ is \textbf{Hermitian}, then $\mat{A}$ is normal.
\end{theorem}
\begin{proof}
    This follows from
    \begin{align*}
        \mat{A}\mat{A}^* = \mat{A} \mat{A} = \mat{A}^* \mat{A}.
    \end{align*}
\end{proof}
\begin{definition}
    A square complex matrix $\mat{A}$ is called \textbf{unitarily diagonalizable} if there exists a unitary matrix $\mat{U}$ and a diagonal matrix $\mat{D}$ such that
    \begin{align*}
        \mat{U}^* \mat{A} \mat{U} = \mat{D}.
    \end{align*}
\end{definition}
\begin{theorem}
    A square complex matrix $\mat{A}$ is unitarily diagonalizable if and only if it is normal.
\end{theorem}
\begin{proof}
    Suppose that $\mat{A}$ is unitarily diagonalizable. Then we can write
    \begin{align*}
        \mat{A} = \mat{U} \mat{D} \mat{U}^*.
    \end{align*}
    Now follows that
    \begin{align*}
        \mat{A}^* \mat{A} &= (\mat{U} \mat{D} \mat{U}^*)^* (\mat{U} \mat{D} \mat{U}^*) \\
        &= (\mat{U} \mat{D}^* \mat{U}^*) (\mat{U} \mat{D} \mat{U}^*) \\
        &= \mat{U} \mat{D}^* \mat{D} \mat{U}^* \\
        &= \mat{U} \mat{D} \mat{D}^* \mat{U}^* \\
        &= (\mat{U} \mat{D} \mat{U}^*) (\mat{U} \mat{D}^* \mat{U}^*) \\
        &= (\mat{U} \mat{D} \mat{U}^*) (\mat{U} \mat{D} \mat{U}^*)^* \\
        &= \mat{A} \mat{A}^*.
    \end{align*}
    So $\mat{A}$ is normal. The other part of the proof is emitted.
\end{proof}

\subsection{Least Squares}
\begin{definition}
    If $W$ is a subspace of a normed linear space $V$ and if $\vec{v}$ is vector in $V$, then the \textbf{best approximation to $\vec{v}$ in $W$} is the vector $\vec{\hat{v}}$ in $W$ such that
    \begin{align*}
        ||\vec{v} - \vec{\hat{v}}|| \le ||\vec{v} - \vec{w}||
    \end{align*}
    for every $\vec{w}$ in $W$.
\end{definition}

\begin{theorem}
    If $W$ is a finite-dimensional subspace of an inner product space $V$ and if $\vec{v}$ is a vector in $V$ then $\proj_W(\vec{v})$ is the best approximation to $\vec{v}$ in $W$.
\end{theorem}
\begin{proof}
    Let $\vec{w}$ be a vector in $W$ different from $\proj_W(\vec{v})$. Then it follows that
    \begin{align*}
        &||\vec{v} - \vec{w}||^2 \\
        &= ||\Perp_W(\vec{v}) + \proj_W(\vec{v}) - \vec{w}||^2 \\
        &= ||\Perp_W(\vec{v})||^2 + 2\langle \Perp_W(\vec{v}), [\proj_W(\vec{v}) - \vec{w}]\rangle + ||\proj_W(\vec{v}) - \vec{w}||^2 \\
        &=||\Perp_W(\vec{v})||^2 + ||\proj_W(\vec{v}) - \vec{w}||^2 \\
        &\ge ||\Perp_W(\vec{v})||^2 \\
        &= ||\vec{v} - \proj_W(\vec{v})||^2.
    \end{align*}
\end{proof}

\begin{definition}
    If $\mat{A}$ is an $m \times n$ matrix and $\vec{b}$ is in $\mathbb{R}^m$, a \textbf{least squares solution} of $\mat{A} \vec{x} = \vec{b}$ is a vector $\vec{\hat{x}}$ in $\mathbb{R}^n$ such that
    \begin{align*}
        ||\vec{b} - \mat{A} \vec{\hat{x}}|| \le ||\vec{b} - \mat{A} \vec{x}||
    \end{align*}
    for all $\vec{x}$ in $\mathbb{R}^n$.
\end{definition}

\begin{theorem}
    Let $\mat{A}$ be an $m \times n$ matrix and let $\vec{b}$ be in $\mathbb{R}^m$. Then $\mat{A} \vec{x} = \vec{b}$ always has at least one least squares solution $\vec{\hat{x}}$. Furthermore, $\vec{\hat{x}}$ is a least squares solutions if and only if
    \begin{align*}
        \mat{A}^T \mat{A} \vec{\hat{x}} = \mat{A}^T \vec{b}.
    \end{align*}
\end{theorem}
\begin{proof}
    Let $\vec{\hat{y}}=\mat{A}\vec{\hat{x}}$. Then $\vec{\hat{y}}$ satisfies
    \begin{align*}
        ||\vec{b} - \vec{\hat{y}}|| \le ||\vec{b} - \vec{y}||
    \end{align*}
    for all $\vec{y}$ in $\col(\mat{A})$. We recognise that $\vec{\hat{y}}$ is the best approximation to $\vec{b}$ in $\col(\mat{A})$. Therefore
    \begin{align*}
        \vec{\hat{y}} = \proj_{\col(\mat{A})}(\vec{b}) = \mat{A}\vec{\hat{x}} = \vec{b} - \Perp_{\col(\mat{A})}(\vec{b}).
    \end{align*}
    So $\mat{A}\vec{\hat{x}} - \vec{b}$ is orthogonal to every vector in $\col(\mat{A})$. Therefore, if $\vec{a}_i$ is the $i$'th column of $\mat{A}$, then
    \begin{align*}
        \vec{a}_i \cdot (\vec{b} - \mat{A} \vec{\hat{x}}) = 0.
    \end{align*}
    This yields
    \begin{align*}
        \mat{A}^T \vec{b} - \mat{A}^T \mat{A} \mat{\hat{x}}=0.
    \end{align*}
\end{proof}

\section{Signal Processing}
% Signal processing:
% SnR
% Matched Filter
% Complex envelope
\subsection{Sampling and Aliasing}
\begin{definition}
    The \textbf{Fourier transform} of a function $x(t)$ is
    \begin{align*}
        \F[x(t)] = \int_{-\infty}^{\infty} x(t) \exp(-j \omega t)dt.
    \end{align*}
\end{definition}
\begin{definition}
    The \textbf{delta impulse train} is given by
    \begin{align*}
        \Sha(t)=\sum_{n=-\infty}^{\infty} \delta(t- n).
    \end{align*}
\end{definition}
\begin{theorem}
    The Fourier transform of the delta impulse train is given by
    \begin{align*}
        \F[\Sha(t)]=\Sha\left(\frac{\omega}{2 \pi}\right).
    \end{align*}
\end{theorem}
\begin{proof}
    Applying the definition of the Fourier transform yields
    \begin{align*}
        \F[\Sha(t)] &= \int_{-\infty}^{\infty} \sum_{n=-\infty}^{\infty} \delta(t- n) \exp(-j \omega t)dt \\
        &= \sum_{n=-\infty}^{\infty}\int_{-\infty}^{\infty}\delta(t- n) \exp(-j \omega t)dt \\
        &= \sum_{n=-\infty}^{\infty} \exp(-j \omega n).
    \end{align*}
    This function is $2 \pi /n$ periodic. Now consider the Fourier transform
    \begin{align*}
        \F[\Sha(t) \ast \rect(t)] &= \F[\Sha(t)] \F[\rect(t)] = \F[\Sha(t)] \sinc\left(\frac{\omega}{2}\right).
    \end{align*}
    Also, note that
    \begin{align*}
        \F[\Sha(t) \ast \rect(t)] =\F[1] = 2 \pi \delta(\omega)= \F[\Sha(t)] \sinc\left(\frac{\omega}{2}\right).
    \end{align*}
    This tells us something about $\F[\Sha(t)]$ where $\sinc(\omega/2 pi)$ is non-zero. More specifically,
    \begin{align*}
        \F[\Sha(t)] = 2 \pi \delta(t)
    \end{align*}
    for $-\pi < \omega \le \pi$ since $\sinc(0)=1$. Here we made use of the uniqueness of the Fourier transform. We now make use of the periodicity of $\F[\Sha(t)]$ to determine that
    \begin{align*}
        \F[\Sha(t)] = \sum_{n=-\infty}^{\infty} 2 \pi \delta(\omega - 2 \pi n)= \sum_{n=-\infty}^{\infty} \delta\left(\frac{\omega}{2\pi} - n\right) = \Sha\left(\frac{\omega}{2\pi}\right).
    \end{align*}
\end{proof}

\begin{definition}
    The signal $x(t)$ \textbf{ideally sampled} is given by
    \begin{align*}
        \tilde{x}(t) = x(t) \cdot \frac{1}{T}\Sha\left(\frac{t}{T}\right).
    \end{align*}
\end{definition}
Note that
\begin{align*}
    \frac{1}{T}\Sha\left(\frac{t}{T}\right) = \frac{1}{T} \sum_{n=-\infty}^{\infty}\delta\left(\frac{\omega}{T} - 2 \pi n\right) = \sum_{n=-\infty}^{\infty} \delta(\omega - 2 \pi n T).
\end{align*}
Also, we can calculate that
\begin{align*}
    \F[\tilde{x}(t)] = \frac{1}{2 \pi} \F[x(t)] \ast \F\left[\frac{1}{T}\Sha\left(\frac{t}{T}\right)\right] = \frac{1}{2 \pi} \F[x(t)] \ast \Sha\left(\frac{\omega T}{2 \pi}\right).
\end{align*}
\begin{definition}
    The frequency $w_s = 2 \pi / T$ is called the \textbf{sampling frequency}.
\end{definition}
Further simplification of $\F[\tilde{x}(t)]$ yields that
\begin{align*}
    \F[\tilde{x}(t)] &= \frac{1}{2 \pi} \F[x(t)] \ast \sum_{n=-\infty}^{\infty} \delta\left(\frac{\omega T}{2\pi} - n\right) \\
    &= \frac{1}{T} \F[x(t)] \ast \sum_{n=-\infty}^{\infty} \delta( \omega - n \omega_s) \\
    &= \frac{1}{T} \sum_{n=-\infty}^{\infty} \left.\F[x(t)]\right|_{w=n \omega_s}.
\end{align*}
Thus sampling replicates $\F[x(t)]$'s spectrum at distances $w_s$. This yields the following theorem.
\begin{definition}
    If $x(t)$ is ideally sampled such that the replicas of the spectrum interfere, then \textbf{aliasing} occurs.
\end{definition}
\begin{theorem}
    Let $x(t)$ be such that $\F[x(t)] = 0$ for $|\omega| \ge \omega_a$. Consider $x(t)$ ideally sampled. Then the following hold.
    \begin{enumerate}
        \item If $w_s \ge 2 w_a$, then no aliasing occurs and $x(t)$ can be reconstructed from $\hat{x}(t)$.
        \item If $w_s \le 2 w_a$, then aliasing may occur.
    \end{enumerate}
\end{theorem}

\subsection{Fourier Transform of Discrete Signals}
We can write $x(t)$ ideally sampled as
\begin{align*}
    \tilde{x}(t) = x(t) \cdot \frac{1}{T}\Sha\left(\frac{t}{T}\right) = \sum_{n=-\infty}^{\infty} x(nT) \delta(t - nT) = \sum_{n=-\infty}^{\infty} x[n] \delta(t - nT).
\end{align*}
Then
\begin{align*}
    \F[\tilde{x}(t)] &= \int_{-\infty}^{\infty} \sum_{n=-\infty}^{\infty} x[n] \delta(t - nT) \exp(-j \omega t) dt \\
    &= \sum_{n=-\infty}^{\infty} x[n] \int_{-\infty}^{\infty}  \delta(t - nT) \exp(-j \omega t) dt \\
    &= \sum_{n=-\infty}^{\infty} x[n] \exp(-j n \Omega)
\end{align*}
where $\Omega = \omega T$.
\begin{definition}
    The \textbf{discrete-time Fourier transform} (DTFT) of a discrete signal $x[n]$ is given by
    \begin{align*}
        \F_*\{x[n]\} = \sum_{n=-\infty}^{\infty} x[n] \exp(-j n \Omega).
    \end{align*}
\end{definition}
Now suppose we have obtained the first $M$ samples of a discrete signal $x[n]$. This can be modelled by
\begin{align*}
    w[n] = x[n]\rect\left(\frac{n}{M} - \frac{1}{2}\right).
\end{align*}
Then
\begin{align*}
    \F_*\{w[n]\} = \frac{1}{2 \pi} \F_*\{x[n]\} \ast \F_*\left\{\rect\left(\frac{n}{M} - \frac{1}{2}\right)\right\}.
\end{align*}
Thus $\F_*\{x[n]\}$ is spread out by convolution with
\begin{align*}
    \F_*\left\{\rect\left(\frac{n}{M} - \frac{1}{2}\right)\right\} = \frac{\sin[\Omega(M+1)/2]}{\sin(\Omega/2)}\exp\left(-\frac{j \Omega M}{2}\right).
\end{align*}
Using finitely many samples to approximate the discrete-time Fourier transform yields the \textbf{discrete Fourier transform}. The \textbf{resolution} of the discrete Fourier transform determines smallest variation in frequency we can detect in the obtained spectrum. This is given by the first zero of the just obtained function. Thus
\begin{align*}
    \Delta \Omega \approx \Omega_0 = \frac{2 \pi}{M+1} \approx \frac{2 \pi}{M}.
\end{align*}

\subsection{Dimensionality Theorem}
The results of the theory of sampling may be stated in a more general way.
\begin{theorem}[Dimensionality]
   A real waveform may be completely specified by $N=2 B T_0$ independent pieces of information that will describe the waveform over a interval of length $T_0$. $N$ is the dimensionality of the waveform and $B$ the absolute bandwidth.
\end{theorem}

\subsection{Complex Envelope}
\begin{theorem}[Complex Envelope]
    Any physical bandpass waveform can be represented by
    \begin{align*}
        x(t) = \Re[g(t) \exp(j \omega_c t)].
    \end{align*}
    $g(t)$ is called the complex envelope of $x(t)$.
\end{theorem}
\begin{theorem}
    The spectrum of $x(t)$ is given by
    \begin{align*}
        \F[x(t)] = \frac{1}{2}[G(f-f_c)+\bar{G}(-f-f_x)]
    \end{align*}
    where $G(f) = \F[g(t)]$.
\end{theorem}
\begin{proof}
    We can write
    \begin{align*}
        x(t) = \Re[g(t) \exp(j \omega_c t)]
        = \frac{1}{2}[g(t) \exp(j \omega_c t) + \bar{g}(t) \exp(-j \omega_c t)].
    \end{align*}
    Therefore
    \begin{align*}
        \F[x(t)] &= \frac{1}{2}\{\F[g(t) \exp(j \omega_c t)] + \F[\bar{g}(t) \exp(-j \omega_c t)]\} \\
        &= \frac{1}{2}[G(f-f_c)+\bar{G}(-f-f_c)].
    \end{align*}
\end{proof}

\begin{theorem}
    The power spectral density of $x(t)$ is given by
    \begin{align*}
        \mathcal{P}_x(f) = \frac{1}{4}[\mathcal{P}_g(f-f_c) + \bar{\mathcal{P}}_g(-f-f_c)]
    \end{align*}
    where $\mathcal{P}_g(f)$ is the power spectral density of $g(t)$.
\end{theorem}
\begin{proof}
    Proof to come.
\end{proof}

\begin{theorem}
    Average power.
\end{theorem}

\subsection{Signal-to-Noise Ratio}
% Definition
% Shannon
% Quantization Noise
% Bit error

\subsection{Matched Filter}
% Derivation
% Bit energy
% Bit probability

\end{document}