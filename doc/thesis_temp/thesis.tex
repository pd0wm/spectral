%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../includes/preamble.tex}
\addbibresource{../includes/bibliography.bib}

\title{Thesis}

\author{W.P. Bruinsma \and R.P. Hes \and H.J.C. Kroep \and T.C. Leliveld \and W.M. Melching \and T.A. aan de Wiel}

\raggedbottom

\begin{document}
\frontmatter

\begin{titlingpage}
  \pagestyle{empty}
  \maketitle
\end{titlingpage}

\tableofcontents

\mainmatter
\chapter{Preliminaries}
A vector is always assumed to be a column vector, unless stated otherwise.

\begin{definition}[Vector Element]
    Let $\vec{x} \in \mathbb{R}^N$. Then $(\vec{x})_i$ denotes the $i$'th element of $\vec{x}$ where $i = 1,\ldots,N$.
\end{definition}

\begin{definition}[Subvector]
    Let $\vec{x} \in \mathbb{R}^N$. Then $\vec{x}[a,b]$ denotes a vector $\vec{z} \in \mathbb{R}^{b-a+1}$ such that $(\vec{z})_i = (\vec{x})_{i+a-1}$ for $i = 1,\ldots,b-a+1$.
\end{definition}

\begin{definition}[Convolution]
    Let $\vec{x} \in \mathbb{R}^N$ and $\vec{y} \in \mathbb{R}^M$. Then $\vec{x} \ast \vec{y}$ denotes a vector $\vec{z} \in \mathbb{R}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{definition}
\begin{theorem}[Commutativity of Convolution] \label{th:conv-comm}
    Let $\vec{x} \in \mathbb{R}^N$ and $\vec{y} \in \mathbb{R}^M$. Then $\vec{x} \ast \vec{y} = \vec{y} \ast \vec{x}$.
\end{theorem}
\begin{definition}[Cross-correlation]
    Let $\vec{x} \in \mathbb{R}^N$ and $\vec{y} \in \mathbb{R}^M$. Then $\vec{x} \circ \vec{y}$ denotes a vector $\vec{z} \in \mathbb{R}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{M-i+k}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{definition}
\begin{theorem} \label{th:corr-unbiased}
    Let $X[n]$ and $Y[n]$ be wide sense stationary stochastic processes. Let $\vec{x} \in \mathbb{R}^N$ and $\vec{y} \in \mathbb{R}^M$ be such that $(\vec{x})_i = X[i]$ and $(\vec{y})_i = Y[i]$. Then $\vec{x} \circ \vec{y}$ is an unbiased estimator of the cross-correlation of $X[n]$ and $Y[n]$ from $1-M$ to $N-1$ windowed by
    \begin{align*}
        \begin{bmatrix}
            1 & \cdots & K - 1 & K & \cdots & K & K - 1 & \cdots & 1
        \end{bmatrix}^T
    \end{align*}
    where $K = \min\{N,M\}$.
\end{theorem}

\chapter{Reconstruction}
\section{Main Analysis}
Let the input signal be denoted by $\vec{x} \in \mathbb{R}^{LN}$. Consider $\vec{c}_i \in \mathbb{R}^{N}$ for $i = 1,\ldots,M$. Let $\vec{y}_i = \vec{c}_i \ast \vec{x}$ for $i = 1,\ldots,M$.
\begin{theorem} \label{th:conv-corr}
    \begin{align*}
        (\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x}) = (\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x}).
    \end{align*}
\end{theorem}
Let $\vec{r}_{y_i,y_j} = \vec{y}_i \circ \vec{y}_j$ and $\vec{r}_{c_i,c_j} = \vec{c}_i \circ \vec{c}_j$. Then
\begin{align*}
    \vec{r}_{y_i,y_j} =(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x}) = (\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x}) = \vec{r}_{c_i,c_j} \ast \vec{r}_x.
\end{align*}
Let $\hat{\vec{r}}_{y_i,y_j} = \vec{r}_{y_i,y_j}[0,2LN-1]$. Then commutativity and the definition of the convolution operator yield that
\begin{align*}
    \hat{\vec{r}}_{y_i,y_j}
    &= \vec{r}_{y_i,y_j}[0,2LN-1] \\
    &= (\vec{r}_{c_i,c_j} \ast \vec{r}_x)[0,2LN-1] \\
    &= (\vec{r}_x \ast \vec{r}_{c_i,c_j})[0,2LN-1] \\
    &= \hskip \textwidth minus \textwidth \\ % Fill line
    \intertext{$\begin{bmatrix}
        (\vec{r}_{c_i,c_j})_1 & 0 & 0& \cdots & &  0 \\
        (\vec{r}_{c_i,c_j})_2 & (\vec{r}_{c_i,c_j})_1 & 0 & \cdots & & 0 \\
        &  & & \vdots &  & \\
        \cdots & (\vec{r}_{c_i,c_j})_{2N-1} & (\vec{r}_{c_i,c_j})_{2N-2} & \cdots & (\vec{r}_{c_i,c_j})_1 & 0 \\
        \cdots & 0 & (\vec{r}_{c_i,c_j})_{2N-1} & \cdots & (\vec{r}_{c_i,c_j})_2 & (\vec{r}_{c_i,c_j})_1 \\
    \end{bmatrix}
    \begin{bmatrix}
        (\vec{r}_x)_1 \\
        (\vec{r}_x)_2 \\
        \vdots \\
        (\vec{r}_x)_{2LN-2} \\
        (\vec{r}_x)_{2LN-1}
    \end{bmatrix}$}
    &= \mat{R}_{c_i,c_j} \vec{r}_x.
\end{align*}
Let the $2L-1\times 2NL-1$ decimation matrix be defined by $(\mat{D})_{i,iN} = 1$ for $i=1,\ldots,2L-1$ and otherwise zero. Then let $\vec{r}_y$ and $\mat{R}$ be such that
\begin{align*}
    \vec{r}_y
    = \begin{bmatrix}
        \mat{D}\hat{\vec{r}}_{y_1,y_1} \\
        \vdots \\
        \mat{D}\hat{\vec{r}}_{y_M,y_M}
    \end{bmatrix}
    = \begin{bmatrix}
        \vec{r}_{y'_1,y'_1} \\
        \vdots \\
        \vec{r}_{y'_M,y'_M}
    \end{bmatrix}
    = \begin{bmatrix}
        \mat{D}\mat{R}_{c_1,c_1} \vec{r}_x \\
        \vdots \\
        \mat{D}\mat{R}_{c_M,c_M} \vec{r}_x
    \end{bmatrix}
    = \begin{bmatrix}
        \mat{D}\mat{R}_{c_1,c_1}\\
        \vdots \\
        \mat{D}\mat{R}_{c_M,c_M}
    \end{bmatrix} \vec{r}_x
    = \mat{R} \vec{r}_x.
\end{align*}
Thus $\mat{R}$ relates $\vec{r}_x$ to $\vec{r}_y$. We now investigate $\vec{r}_{y'_i,y'_j}$'s structure. Notice that
\begin{align*}
    (\vec{r}_{y'_i,y'_j})_m = (\mat{D} \hat{\vec{r}}_{y_i,y_j})_{m} = (\hat{\vec{r}}_{y_i,y_j})_{mN} = (\vec{r}_{y_i,y_j})_{mN}.
\end{align*}
Thus $\vec{r}_{y'_i,y'_j}$ is the $N$-decimation of $\vec{r}_{y_i,y_j}$. Let $\vec{y}'_i$ denote the $N$-decimation of $\vec{y}_i$ for $i = 1, \ldots, M$.
\begin{theorem} \label{th:deci-corr}
   Then
    \begin{align*}
        \frac{2LN-1}{2L-1}\vec{y}'_i \circ \vec{y}'_j
    \end{align*}
   is an unbiased estimator of $E(\vec{r}_{y'_i,y'_j})$.
\end{theorem}
Finally, since $\vec{c}_i \in \mathbb{R}^N$
\begin{align*}
    (\vec{y}'_i)_m = (\vec{y}_i)_{mN} = (\vec{c}_i \ast \vec{x})_{mN} = \hat{\vec{c}}_i \cdot \vec{x}[(m-1)N+1,mN]
\end{align*}
where $\hat{\vec{c}}_i$ denotes $\vec{c}_i$ reversed. This concludes the main analysis.

\section{The Algorithm}

\section{Exploring Further Possibilities}
\subsection{Sample-Wide Autocorrelation}

\subsection{Unbiased Estimate}
Unbiased PSD estimate? But mean square error worse! Wikipedia reference. Mean square error calculation? Describe as dewindowing

\chapter{Proofs}
\begin{proof}[Proof of \cref{th:conv-comm}]
    A change of index yields that
    \begin{align*}
        (\vec{x} \ast \vec{y})_i &= \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k=-\infty}^{\infty} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k'=-\infty}^{\infty} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= \sum_{k'=1}^{M} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= (\vec{y} \ast \vec{x})_i.
    \end{align*}
\end{proof}
\begin{proof}[Proof of \cref{th:corr-unbiased}]
    Without loss of generality, assume that $M \le N$. Suppose that $i \le M$, then
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= \sum_{k=1}^N E[(\vec{x})_k (\vec{y})_{M-i+k}] \\
        &= E[(\vec{x})_1 (\vec{y})_{M-i+1}] + \ldots + E[(\vec{x})_i (\vec{y})_{M}] \\
        &= E(X[1] Y[M-i+1]) + \ldots + E(X[i] Y[M])  \\
        &= i R_{X,Y}(i-M).
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. Now suppose that $M < i \le N$, then
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= E[(\vec{x})_{i-M+1} (\vec{y})_{1}] + \ldots + E[(\vec{x})_i (\vec{y})_{M}] \\
        &= E(X[i-M+1] Y[1]) + \ldots + E(X[i] Y[M]) \\  
        &= M R_{X,Y}(i-M).
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. Finally suppose that $i > N$, yhen
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= E[(\vec{x})_{i-M+1} (\vec{y})_{1}] + \ldots + E[(\vec{x})_N (\vec{y})_{M-i+N}] \\
        &= E(X[i-M+1] Y[1]) + \ldots + E(X[N] Y[M-i+N]) \\
        &= (N+M-i) R_{X,Y}(i-M).
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. So
    \begin{align*}
        E(\vec{x} \circ \vec{y}) &= \begin{bmatrix}
            E[(\vec{x} \circ \vec{y})_1] \\
            E[(\vec{x} \circ \vec{y})_2] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_{M-1}] \\
            E[(\vec{x} \circ \vec{y})_{M}] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_N] \\
            E[(\vec{x} \circ \vec{y})_{N+1}] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_{N+M-2}] \\
            E[(\vec{x} \circ \vec{y})_{N+M-1}]
        \end{bmatrix} \\
        &= \begin{bmatrix}
            R_{X,Y}(1-M) \\
            2 R_{X,Y}(1-M) \\
            \vdots \\
            (M-1) R_{X,Y}(-1) \\
            M R_{X,Y}(0) \\
            \vdots \\
            M R_{X,Y}(N-M) \\
            (M-1) R_{X,Y}(N+1-M) \\
            \vdots \\
            2 R_{X,Y}(N-2) \\
            R_{X,Y}(N-1)
        \end{bmatrix} \\
        &= \begin{bmatrix}
            1 \\
            2 \\
            \vdots \\
            M-1 \\
            M \\
            \vdots \\
            M \\
            M-1 \\
            \vdots \\
            2 \\
            1
        \end{bmatrix} \cdot \begin{bmatrix}
            R_{X,Y}(1-M) \\
            R_{X,Y}(1-M) \\
            \vdots \\
            R_{X,Y}(-1) \\
            R_{X,Y}(0) \\
            \vdots \\
            R_{X,Y}(N-M) \\
            R_{X,Y}(N+1-M) \\
            \vdots \\
            R_{X,Y}(N-2) \\
            R_{X,Y}(N-1)
        \end{bmatrix}.
    \end{align*}
    %     &= \sum_{k=1}^N E(X[k] Y[M-i+k]) \\
    %     &= \sum_{k=1}^{N+M-1} E(X[k] Y[M-i+k]).
    % \end{align*}
    % Since $X[i]$ and $Y[i]$ are wide sense stationary stochastic processes,
    % \begin{align*}
    %     E[(\hat{\vec{\theta}})_i] &= \frac{1}{N+M-1} \sum_{k=1}^{N+M-1} R_{X,Y}(M-i) \\
    %     &= R_{X,Y}(M-i).
    % \end{align*}
\end{proof}
\begin{proof}[Proof of \cref{th:conv-corr}]
    Note that
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        &= [(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x})]_m \\
        &=\sum_{k''=1}^{LN+N-1}\sum_{k=1}^N (\vec{c}_i)_k (\vec{x})_{k''-k+1}\sum_{k'=1}^{N}(\vec{c}_j)_{k'}(\vec{x})_{(LN+N-1-m+k'')-k'+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{k'=\infty}^{\infty}\sum_{k''=-\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{k'}(\vec{x})_{k''-k+1}(\vec{x})_{(LN+N-1-m+k'')-k'+1}.
    \end{align*}
    To further evaluate this expression, we introduce a change of variables. To this end, let $k' = N -l'' +k$ and $k'' = l' + k - 1$. This transformation is invertible, so
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        % I don't think this step is necessary
        % &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}(\vec{x})_{(l' + k - 1)-k+1}(\vec{x})_{[LN+N-1-m+(l' + k - 1)]-(N -l'' +k)+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}(\vec{x})_{l'}
        (\vec{x})_{LN-(m-l'' + 1)+l'} \\
        &=\sum_{l''=1}^{2N-1}\sum_{k=1}^{N}(\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}\sum_{l'=1}^{LN}(\vec{x})_{l'}
        (\vec{x})_{LN-(m-l'' + 1)+l'} \\
        &=[(\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x})]_m.
    \end{align*}
\end{proof}
\begin{proof}[Proof of \cref{th:deci-corr}]
    Note that
    \begin{align*}
        \frac{2LN-1}{2L-1} E[(\vec{y}'_i \circ \vec{y}'_j)_m]
        &= \frac{2LN-1}{2L-1} \sum_{k=1}^{L}E[(\vec{y}'_i)_k (\vec{y}'_j)_{L-m+k}] \\
        &= \frac{2LN-1}{2L-1} \sum_{k=1}^{L}E[(\vec{y}_i)_{kN} (\vec{y}_j)_{LM-mN+kN}] \\
    \end{align*}
\end{proof}

\end{document}
