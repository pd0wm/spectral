%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../../includes/preamble.tex}
\addbibresource{../../../../../includes/bibliography.bib}

\begin{document}

\section{Energy detector using the reconstructed autocorrelation}\label{sec:ari_ed_deriv}
In this section we will derive how $\gamma_k$ as introduced in \cref{ssec:ari_ed} can be calculated. This derivation is largely based on \cite{ariananda2012compressive} and is heavily dependent on the derivation in \cref{sec:reconstruction-derivation}.

We assume that if our signal $x[n]$ purely contains noise, then $x[n] \sim \mathcal{CN}(0,\sigma_n^2)$. 
Let
\begin{align*}
    \vec{\hat{r}}_x = \begin{bmatrix}\hat{r}[-LN]& \cdots& \hat{r}[0]& \cdots & \hat{r}[LN]\end{bmatrix}^T
\end{align*}
denote the vector containing the estimated elements of the autocorrelation. 
By \cref{eq:pseudo-rx} we know that the estimator $\vec{\hat{r}}_x$ of $\vec{r}_x$ is given as
\begin{align*}
\vec{\hat{r}}_x &= \mat{R}^{\dagger}\vec{\hat{r}}_y.
\end{align*} where the elements of the estimator $\vec{\hat{r}}_y$ of $\vec{r}_y$ are given by \cref{eq:ryij-est}.

Let $\mat{F}$ denote the $(2LN+1) \times (2LN+1)$ discrete Fourier transform matrix, then $\vec{\hat{s}}_x$, defined as
$\vec{\hat{s}}_x = \mat{F} \vec{\hat{r}}_x$
is an estimator of the power spectral density $\mathcal{P}(\omega)$ at frequencies $\omega = 2\pi k/(2LN+1)$ where $0 \leq k \leq 2LN$ .

In \cite{ariananda2012compressive} it is derived that when $KLN$ is large, then the elements of $\vec{s}_x$ are approximately Gaussian distributed. Let $\hat{s}_k$ denote the $k$'th element of $\vec{\hat{s}}_x$, then we approximate its distribution by the Gaussian $\mathcal{N}(\mu_k, \sigma^2_k)$.

The threshold $\gamma_k$ is then given by 
\begin{align}\label{eq:ari_threshold}
\gamma_k = Q^{-1}(p\ss{fa})\sigma_{k} + \mu_{k}.
\end{align}

Note that $\mu_k$ is equal to the $k$'th element of the vector
\begin{align*}
E\left(\vec{\hat{s}}_x\right) &= \mat{F}\mat{R}^{\dagger}E(\vec{r}_y).
\end{align*}

Making use of the following relation as derived in \cite{ariananda2012compressive}, giving  the elements of $E(\vec{r}_y)$ 
\begin{align*}
E(\vec{r}_{y_i, y_j}[k]) &= E[(c_{i,j} \ast r_x) (kN)] & \text{\cref{eq:crij-rx}} \\
&= c_{i,j} \delta[k]
\end{align*}
we can calculate $\mu_k$ for $1 \leq k \leq 2LN+1$.

To calculate $\sigma_k$ we note that the $k$'th element on the diagonal of the covariance matrix of $\vec{\hat{s}}_x$ contains the variance $\sigma_k^2$. Let $\mat{C}_{\hat{s}_x}$ denote the covariance matrix of $\vec{\hat{s}}_x$.  We can compute $\mat{C}_{\hat{s}_x}$ by

\begin{align}\label{eq:terror_cov_sx}
\mat{C}_{\hat{s}_x} &= E(\vec{\hat{s}}_x\vec{\hat{s}}_x^H) - E(\vec{\hat{s}})E(\vec{\hat{s}}^H) \\
&= E[(\mat{F} \mat{R}^{\dagger}\vec{\hat{r}}_y) ( \mat{F}\mat{R}^{\dagger}\vec{\hat{r}}_y)^H] - E(\mat{F}\mat{R}^{\dagger}\vec{\hat{r}}_y)E[(\mat{F}\mat{R}^{\dagger}\vec{\hat{r}}_y)^H] \nonumber \\
&= \mat{F}\mat{R}^{\dagger} E(\vec{r}_y\vec{r}_y^H)  \mat{R}^{\dagger H}\mat{F}^H -  \mat{F}\mat{R}^{\dagger} E( \vec{r}_y) E(\vec{r}_y^H)  \mat{R}^{\dagger H}\mat{F}^H \nonumber \\
&= \mat{F}\mat{R}^{\dagger} \mat{C}_{r_y} \mat{R}^{\dagger H}\mat{F}^H.\nonumber
\end{align}

Let $\mat{C}_{\hat{r}_y}$ denote the covariance matrix of of $\vec{\hat{r}}_y$, with its elements given by

\begin{align}\label{eq:cov_ry}
\text{Cov}(\hat{r}_{y_t,y_u}[i],\hat{r}_{y_v, y_w}[j]).
\end{align}

As stated in \cite{ariananda2012compressive}, under the assumption that the elements of $x[n]$ are independent identically circular complex Gaussian distributed, the expression for the covariance in \cref{eq:cov_ry} simplifies to

\begin{align}\label{eq:elem_cov_ry}
\text{Cov}(\hat{r}_{y_t,y_u}[i],\hat{r}_{y_v, y_w}[j]) &= \frac{\sigma_n^4 c_{t,v}[0] \conj{c}_{u,w}[0] \delta \left[ j-i\right]}{KL-|i|}.
\end{align}

Now that we have described how to calculate $\mu_k$ and $\sigma_k$, \cref{eq:ari_threshold} can be evaluated.

% <<<<<<< HEAD
% As $\vec{c}_i$ and $\vec{c}_j$ are constant vectors, we will only have to focus on the last term. Let us introduce the helper variable $a =  \sum_{k=1}^{KL} (\vec{x})_{kN-l+1} \cdot (\vec{\overline{x}})_{(k-u+L)N - m+1}$. Notice that if

% \begin{enumerate}
% 	\item $u=L, l=m$, then $a = \sum_{k=1}^{KL}\vec{x}_{kN-l+1}\vec{\overline{x}}_{kN-l+1}$. That is, $a$ follows a $\chi^2$ distribution (by definition). As with the convential energy detector, we can approximate the distribution of the sum as a whole as gaussian if $KL$ is large enough.
% 	\item $l \neq m$. Notice that as 
% 	% \begin{align*}\{(\vec{x})_{kN-l+1} : 1 \leq k \leq KL\} \cap 
% 	% \{(\vec{x})_{(k-u+L)N - m+1} : 1 \leq k \leq KL\}  &= \emptyset
% 	% \end{align*}

% 	\begin{align*}
% 	\bmod(kN-l+1,N) &= \bmod(-l+1, N)
% 	\end{align*}
% 	and
% 	\begin{align*}
% 	\bmod((k-u+L)N - m+1,N) &= \bmod(-m+1, N)
% 	\end{align*}
% 	the product in the sum
% 	That is $(\vec{x})_{kN-l+1}$  can never equal  $(\vec{x})_{(k-u+L)N - m+1}$ in $a$. Because each element of $\vec{x}$ has the same distribution, the product of two distinct elements in $\vec{x}$ will always yield the same distribution (whatever it may be). By the Central Limit Theorem we can approximate the distribution of the sum as a whole as gaussian if $KL$ is large enough.

% 	\item $u\neq L, l=m$ In this case, the product in the sum contains elements of $\vec{x}$ with a minimum displacement of $N$ in their indices. Unlike case 2, the specified sets \emph{can} have elements in common. To see why, consider \cref{fig:ex_dep}. Notice, that the dependence of a product in the sum on other products is limited: for each element of $\vec{x}$ there will be a maximum of two terms in $a$ dependent on that element. Notice that as $u\neq L$ the first $L-u$ multiplicands in $a$ can \emph{never} appear as multiplier in $a$. The first $L-u$ multipliers, however \emph{can} appear as multiplicand in $a$.  Using this observation we rewrite $a$ using two sums, which alternatingly sum $L-u$ consecutive terms (which guarantees that the multiplicands in the sum can never equal the multiplier) of the original sum in $a$: 

% 	\begin{align*}
% 	a &= \sum_{q=1}^{\left\lfloor{\frac{KL}{2(L-u)}}\right\rfloor} \sum_{r=1}^{L-u} \vec{x}_{((2q-1)(L-u)+r)N-l+1}\vec{\overline{x}}_{((2q-1)(L-u)+r)N-l+1} \\
% 	   & \;+ \sum_{s=1}^{\left\lfloor{\frac{KL}{2(L-u)}}\right\rfloor} \sum_{t=1}^{L-u} \vec{x}_{(2s(L-u)+t)N-l+1}\vec{\overline{x}}_{(2s(L-u)+t)N-l+1}
% 	\end{align*}
% 	Note that we tacitly assumed that $L-u \geq 0$. This does not pose a problem as similar analysis for $L-u < 0$ is possible. 
% 	Therefore, if $\left\lfloor{\frac{KL}{2(L-u)}}\right\rfloor$ is large enough, $a$ is the sum of two approximately gaussian distributed variables, and therefore itself is approximately gaussian distributed.			
% \end{enumerate}

% \begin{figure}
% \begin{tikzpicture}
% \node (species1) {
% \begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
%     $(\vec{x})_{2N-l+1}$ & $\ldots$  & $(\vec{x})_{3N-l+1}$ & $\ldots$ & $(\vec{x})_{4N-l+1}$ & $\ldots$ & $(\vec{x})_{5N-l+1}$  & $\ldots$ \\ \hline 
%     \end{tabular}
  
% };
% \node (species2) [below right= 0.2cm and -14.5cm of species1] {
%       \begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
%     $(\vec{x})_{N-l+1}$ & $\ldots$  & $(\vec{x})_{2N-l+1}$ & $\ldots$ & $(\vec{x})_{3N-l+1}$ & $\ldots$ & $(\vec{x})_{4N-l+1}$  & $\ldots$ \\ \hline 
%     \end{tabular}
% };
% \end{tikzpicture}
% \caption{Example illustrating dependence, $u=L-1$}
% \label{fig:ex_dep}
% \end{figure}
% =======
% Now that we have analyzed how to compute the variance of each element of $\vec{s}_x$, we will analyze the expression of their expected value ($\mu$).
% >>>>>>> ce205588a6e12dc5693c1b9458a8c72e17f44173
\end{document}