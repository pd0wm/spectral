%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../../includes/preamble.tex}
\addbibresource{../../../../../includes/bibliography.bib}

\begin{document}

\section{Preliminaries}
The definitions given in this section are used in the derivations later on and are based  on \cite{yates2005probability,gallager2008circularly}.

\begin{blockDefinition}[Complex Gaussian Random Variable]
Given the complex gaussian random variable $Z = X + jY$, then $Z$ is said to be complex gaussian distributed if $X$ and $Y$ are jointly-gaussian distributed. We denote $Z \sim \mathcal{CN}(\mu, \Gamma, C)$.
\end{blockDefinition}

 A complex gaussian random variable is characterised by three parameters:
\begin{enumerate}
    \item $\mu = E(Z)$,
    \item $\Gamma = E[(Z-\mu)(\conj{Z}-\conj{\mu})^T]$ and
    \item $C = E[(Z-\mu)(Z-\mu)^T]$.
\end{enumerate}

\begin{blockDefinition}[Circular Symmetric Complex Gaussian Random Variable]
Given a complex gaussian random variable $Z = X + jY$, then $Z$ is circular symmetric complex gaussian distributed if $Z$ has the same distribution as $Ze^{j\theta}$ with $\theta$ real. A circular complex gaussian random variable $Z$ is referred to as $Z \sim \mathcal{CN}(0,\sigma^2)$.
\end{blockDefinition}

Since
\begin{align*}
    E(Z) = E(e^{j\theta}Z)  = e^{j\theta}E(Z)
\end{align*}
it follows that $Z$ has an expectation value of zero. To see why let$E(Z) = a + bj$ and notice that $e^{j\theta} \neq 0$.
Then if $e^{j\theta} E(Z) = E(z)$ holds, then so does $(a+bj) = e^{j\theta}(a+bj)$. Now it folllows cannot be anything else than zero.
Furthermore, since
\begin{align*}
    E(ZZ) = E(e^{j\theta}Z e^j\theta{Z})  = e^{2j\theta}E(Z^2)
\end{align*}
% ($E(Z) = 0$) and that %$E(Z^2)= E(X^2 - Y^2 + 2jXY] = 0$.
it follows that $\text{Var}\left(X\right) = \text{Var}\left(Y\right)$ (by an analog argument as for the expectation value). That is, $X$ and $Y$ are independent gaussian distributed with zero mean and equal variance.  It now follows that $\sigma^2= E(Z\overline{Z}) = \text{Var}\left(X\right) + \text{Var}\left(Y\right)$.

\begin{blockDefinition}[Circular Complex Gaussian Random Vector]
Given the random vector $\vec{Z} \in \mathbb{C}^N$, then $\vec{Z} = \vec{X} + j\vec{Y}$ is a circular complex random vector if $\vec{X}$ and $\vec{Y}$ are jointly gaussian distributed and $\vec{Z}$ has the same distribution as $e^{j\theta}\vec{Z}$ with $\theta$ real.
\end{blockDefinition}

Similar to a circular complex gaussian variable, $E(\vec{Z}) = \mat{0}$
and $E(\vec{Z}\vec{Z}^T) = \mat{0}$\cite{gallager2008circularly}. A circular complex gaussian random vector $\vec{Z}$ is referred to as $\vec{Z} \sim \mathcal{CN}(\mat{0},\mat{\Gamma},\mat{0})$ with $\mat{\Gamma} = E(\vec{Z}\conj{\vec{Z}} )$. For the standard circular complex gaussian random vector, $\mathbf{\Gamma} = \mat{I}$. The probability density function of a circular complex gaussian vector $\vec{Z}\in \mathbb{C}^N$ is given by

\begin{align*}
    \frac{1}{\pi^N \text{det}(\mat{\Gamma})} \exp \left(-\conj{(\vec{z}^T)} \mat{\Gamma}^{-1}\vec{z}\right).
\end{align*}

\begin{blockDefinition}[Chi-square distribution for complex random variables]
Given a random vector $\vec{Z} \in \mat{C}^N$, with $Z_i \sim \mathcal{CN}(\mat{0}, 2\mat{I},\mat{0})$ then the random variable $\vec{X}$ defined as

\begin{align*}
	\mathbf{X} &= \sum_{n=1}^N \left|(\vec{Z})_n\right|^2 % http://dsp-book.narod.ru/DSPMW/60.PDF
\end{align*}

% % https://books.google.nl/books?id=KwkgAwAAQBAJ&pg=PA158&lpg=PA158&dq=circular+complex+gaussian+chi+square&source=bl&ots=9e7czQCFaN&sig=yuMXCjiFC21c_0EgmSM_yzefVFk&hl=nl&sa=X&ei=MbB2VfTXBsizswGi34DgCw&ved=0CFIQ6AEwBg#v=onepage&q=circular%20complex%20gaussian%20chi%20square&f=false

% % https://books.google.nl/books?id=ERLrAQAAQBAJ&pg=PA145&dq=circular+complex+gaussian&hl=nl&sa=X&ei=YLd2Vc7HLMmmsgHhx4TgCQ&ved=0CCEQ6AEwAA#v=onepage&q=circular%20complex%20gaussian&f=false

% % http://lib.tkk.fi/Diss/2010/isbn9789526030319/article6.pdf

% % https://www.ee.iitb.ac.in/~sarva/courses/EE703/2013/Slides/CircularlySymmetricGaussian.pdf

% % http://www.ifp.illinois.edu/~pramodv/Chapters_PDF/Fundamentals_Wireless_Communication_AppendixA.pdf

follows a chi-square distribution with $2N$ degrees of freedom, denoted by $\mathbf{X} \sim \chi^2_{2N}$.
$E[X] = 2N$ and $\text{Var}[X] = 4N$.
\end{blockDefinition}

\begin{blockDefinition}[Likelihood function under Hypothesis]
Given a hypothesis $\mathcal{H}$ and a realisation $\mathbf{x}$ of a random variable $\mathbf{X}$, then $L_{\mathbf{X} | \mathcal{H}}(\mathbf{x})$ denotes the likelihood function of $\mathbf{x}$ given $\mathcal{H}$.
\end{blockDefinition}

\begin{blockDefinition}[Neyman-Pearson Test]
Given a continous random vector $\vec{X}$, and two hypotheses $\mathcal{H}_0$ and $\mathcal{H}_1$, the Neyman-Pearson test rejects that the realization $\vec{x}$ of $\vec{X}$, has been produced under $\mathcal{H}_0$ in favor of $\mathcal{H}_1$
when
\begin{align*}
    \Lambda (\mathbf{x}) &= \frac{L_{\vec{X} | \mathcal{H}_0} (\mathbf{x})}{L_{\vec{X} | \mathcal{H}_1}(\mathbf{x})} \leq \gamma.
\end{align*}
where $\gamma$, the decision threshold, is chosen such that $P[\Lambda(\vec{x} < \gamma) | \mathcal{H}_1]$ is minimized subject to $P[(]\Lambda(\vec{x}) > \gamma) | \mathcal{H}_0] = P\ss{fa}$, where $P\ss{fa}$ denotes the false alarm probability. % insert ref naar boek
\end{blockDefinition}

\clearpage
\begin{blockDefinition}[Covariance Matrix]
Given a vector $\vec{x}$, its covariance matrix is defined as $C_{x} = E(\vec{x}\vec{x}^H)-E(\vec{x})E(\vec{x}^H)$
\end{blockDefinition}
\end{document}
