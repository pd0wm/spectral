%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../includes/preamble.tex}
\addbibresource{../../includes/bibliography.bib}

\title{Compressive Sensing - An Overview}

\author{W.P. Bruinsma \and R.P. Hes \and H.J.C. Kroep \and T.C. Leliveld \and W.M. Melching \and T.A. aan de Wiel}

\raggedbottom

\allowdisplaybreaks
\usepackage{float}
\usepackage{tabularx}
\newcommand{\lab}[1]{\label{#1}\nolinebreak}
\begin{document}

\chapter{Reconstruction}

\section{Preliminaries}
Vectors and matrices are denoted by bold-faced letters. Unless stated otherwise, a vector is always assumed to be a column vector. The complex conjugate of a vector $\vec{x}$ is denoted by $\bar{\vec{x}}$. The inner product of an inner product space is denoted by $\cdot$. We introduce notation to denote elements of vectors and matrices.

\begin{blockDefinition}[Vector Element]
    Let $\vec{x} \in \mathbb{C}^N$. Then $(\vec{x})_i$ denotes the $i$'th element of $\vec{x}$ for $i = 1,\ldots,N$.
\end{blockDefinition}

\begin{blockDefinition}[Matrix Element]
    Let $\mat{A}$ be an $M \times N$ matrix. Then $(\mat{A})_{i,j}$ denotes the $j$'th element of the $i$'th row of $\mat{A}$ for $i = 1,\ldots,M$ and $j=1,\ldots,N$.
\end{blockDefinition}

We will need notation to denote two uncommon vector operations.

\begin{blockDefinition}[Subvector]
    Let $\vec{x} \in \mathbb{C}^N$. Then $\vec{x}[a,b]$ denotes a vector $\vec{z} \in \mathbb{C}^{b-a+1}$ such that $(\vec{z})_i = (\vec{x})_{i+a-1}$ for $i = 1,\ldots,b-a+1$.
\end{blockDefinition}

\begin{blockDefinition}[Reverse of Vector]
    Let $\vec{x} \in \mathbb{C}^N$. Then the reverse of $\vec{x}$ denotes a vector $\vec{y} \in \mathbb{C}^N$ such that $(\vec{y})_i = (\vec{x})_{N-i+1}$ for $i = 1,\ldots,N$.
\end{blockDefinition}

We now define the convolution operator as a closed binary operation on vectors. The definition is similar to the definition of convolution for discrete signals. This implies that results similar to well-known theorems can be obtained.

\begin{blockDefinition}[Convolution]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \ast \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{blockDefinition}

\begin{blockTheorem}[Commutativity of Convolution] \label{th:conv-comm}
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \ast \vec{y} = \vec{y} \ast \vec{x}$.
\end{blockTheorem}

We also define correlation as a closed binary operation on vectors. The definition is again similar to the definition of correlation for discrete signals.

\begin{blockDefinition}[Deterministic Cross-correlation]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \circ \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\conj{\vec{y}})_{M-i+k}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{blockDefinition}

Furthermore, we define the Hadamard product as a closed binary operation on vector.

\begin{blockDefinition}[Hadamard Product]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^N$. Then $\vec{x} \odot \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^N$ such that $(\vec{z})_i = (\vec{x})_i (\vec{y})_i$ for $i = 1,\ldots,N$.
\end{blockDefinition}

Note that the notation $\cdot$ of the inner product is similar to the notation $\odot$ of the Hadamard product. The following theorem identifies the expectation value of the correlation operator.

\begin{blockTheorem} \lab{th:corr-unbiased}
    Let $X[n]$ and $Y[n]$ be wide sense stationary stochastic processes. Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$ be such that $(\vec{x})_i = X[i]$ for $i=1,\ldots,N$ and $(\vec{y})_i = Y[i]$ for $i=1,\ldots,M$. Then $\vec{x} \circ \vec{y}$ is an unbiased estimator of the cross-correlation of $X[n]$ and $Y[n]$ from $-M+1$ to $N-1$ weighted by the elements of the vector
    \begin{align*}
        \begin{bmatrix}
            1 & \cdots & K - 1 & K & \cdots &K & K - 1 & \cdots & 1
        \end{bmatrix}^T
    \end{align*}
    where $K = \min\{N,M\}$.
\end{blockTheorem}

Finally, we extend the use of dots to denote finite sequences.

\begin{blockDefinition}
    Let $N \in \mathbb{N}$. Then $(1,1),\ldots,(N,N)$ denotes

    \makebox[\textwidth]{\centering
        $(1,1),\ldots,(1,N),(2,1),\ldots,(2,N),(3,1),\ldots,(N,N).$
    } \nolinebreak
\end{blockDefinition}

Note that the sequence $(1,1),\ldots,(N,N)$ has length
\begin{align*}
    1 + 2 + \ldots + N = \frac{1}{2} N(N+1).
\end{align*}

\section{Overview of Variables}
\Cref{tab:overview-vars-main-analysis} presents an overview of the variables which will be used in the main analysis. The meaning of the name of each variable will become clear in the main analysis.

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{Xlp{3cm}}
        \textbf{Name} & \textbf{Notation} & \textbf{Size} \\ \hline
        Number of samples & $L$ & $1$ \\
        Downsampling factor & $N$ & $1$ \\
        Number of cosets & $M$ & $1$ \\
        Input signal & $\vec{x}$ & $LN$ \\
        $m$'th interval of $N$ samples of the input signal & $\vec{x}_m$ & $N$ \\
        Deterministic correlation of input & $\vec{r}_x$ & $2LN - 1$ \\
        Sampling vector of coset $i$ & $\vec{c}_i$ & $N$ \\
        Sampling vector of coset $i$ reversed & $\vec{d}_i$ & $N$ \\
        Convolution matrix of cosets $i$ and $j$ & $\mat{R}_{c_i,c_j}$ & $2LN-2N-3 \times 2LN-1$ \\
        Deterministic correlation of sampling vectors of cosets $i$ and $j$ & $\vec{r}_{c_i,c_j}$  & $2N-1$ \\
        Pseudo output of coset $i$ & $\vec{y}_i$ & $LN + N - 1$ \\
        Deterministic correlation of pseudo outputs of cosets $i$ and $j$ & $\vec{r}_{y_i,y_j}$  & $2LN + 2N - 3$ \\
        Decimated deterministic correlation of pseudo outputs of cosets $i$ and $j$ & $\vec{r}'_{y_i,y_j}$  & $2L-1$ \\
        Output of coset $i$ & $\vec{y}'_i$ & $L$ \\
        Deterministic correlation of outputs of cosets $i$ and $j$ & $\vec{r}_{y'_i,y'_j}$ & $2L-1$ \\
        Stacked convolution matrices of cosets & $\vec{R}$ & $\frac{1}{2}M(M+1) (2L - 1) \times (2LN - 1)$ \\
        Decimation matrix & $\mat{D}$ & $2L-1 \times 2LN + 2N - 3$ \\
        Stacked deterministic correlation of outputs of cosets & $\vec{r}'_y$ & $\frac{1}{2}M(M+1)(2L-1)$ \\
        Output of all cosets in interval $m$ & $\vec{w}_m$ & $\frac{1}{2}M(M+1)$
    \end{tabularx}
    \caption{Overview of the variables used in the main analysis}
    \label{tab:overview-vars-main-analysis}
\end{table}


\section{Main Analysis}

Let $L$, $N$ and $M$ be parameters. $M$ cosets will provide $L$ samples downsampled by a factor $N$. Thus $LN$ samples of the input signal are required. Let the input signal be denoted by $\vec{x} \in \mathbb{C}^{LN}$. Let for coset $i$ the sampling vector be given by $\vec{c}_i \in \mathbb{C}^{N}$. The sampling vector determines the output of a coset. The exact relationship has yet to be derived. Let for coset $i$ the pseudo output be given by $\vec{y}_i = \vec{c}_i \ast \vec{x}$. The pseudo output will be used to derive the output of a coset.

\begin{blockTheorem} \label{th:conv-corr}
    \makebox[\textwidth]{\centering $(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x}) = (\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x})$.} \nolinebreak
\end{blockTheorem}

Let the correlation of the pseudo outputs of cosets $i$ and $j$ be given by $\vec{r}_{y_i,y_j} = \vec{y}_i \circ \vec{y}_j$. Let the correlation of sampling vectors of cosets $i$ and $j$ be given by $\vec{r}_{c_i,c_j} = \vec{c}_i \circ \vec{c}_j$. Then
\begin{align*}
    \vec{r}_{y_i,y_j} =(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x}) = (\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x}) = \vec{r}_{c_i,c_j} \ast \vec{r}_x.
\end{align*}
Commutativity and the definition of the convolution operator yield that
\begin{align*}
    \vec{r}_{y_i,y_j}
    &= \vec{r}_{c_i,c_j} \ast \vec{r}_x\\
    &= \vec{r}_x \ast \vec{r}_{c_i,c_j} \\
    &= \hskip \textwidth minus \textwidth \\ % Fill line
    \intertext{$\begin{bmatrix}
        (\vec{r}_{c_i,c_j})_1 & 0 & 0& \cdots & 0 \\
        (\vec{r}_{c_i,c_j})_2 & (\vec{r}_{c_i,c_j})_1 & 0 & \cdots & 0 \\
        &  & \ddots &  & \\
        0 &  \cdots & 0 & (\vec{r}_{c_i,c_j})_{2N-1} & (\vec{r}_{c_i,c_j})_{2N-2} \\
        0 &  \cdots & 0& 0 & (\vec{r}_{c_i,c_j})_{2N - 1} \\
    \end{bmatrix}
    \begin{bmatrix}
        (\vec{r}_x)_1 \\
        (\vec{r}_x)_2 \\
        \vdots \\
        (\vec{r}_x)_{2LN-2} \\
        (\vec{r}_x)_{2LN-1}
    \end{bmatrix}$}
    &= \mat{R}_{c_i,c_j} \vec{r}_x.
\end{align*}
Let the $2L-1\times 2LN-2N-3$ decimation matrix be defined by $(\mat{D})_{i,iN} = 1$ for $i=1,\ldots,2L-1$ and otherwise zero. The decimation matrix is used to derive the output of a coset from its pseudo output. Let the decimated correlation of the pseudo outputs of cosets $i$ and $j$ by given by $\vec{r}'_{y_i,y_j} = \mat{D} \vec{r}_{y_i,y_j}$. Then let $\mat{R}$ be such that
\begin{align*}
    \begin{bmatrix}
        \vec{r}'_{y_1,y_1} \\
        \vdots \\
        \vec{r}'_{y_M,y_M}
    \end{bmatrix}
    = \begin{bmatrix}
        \mat{D}\mat{R}_{c_1,c_1} \vec{r}_x \\
        \vdots \\
        \mat{D}\mat{R}_{c_M,c_M} \vec{r}_x
    \end{bmatrix}
    = \begin{bmatrix}
        \mat{D}\mat{R}_{c_1,c_1}\\
        \vdots \\
        \mat{D}\mat{R}_{c_M,c_M}
    \end{bmatrix} \vec{r}_x
    = \mat{R} \vec{r}_x.
\end{align*}
We now investigate $\vec{r}'_{y_i,y_j}$'s structure. Notice that
\begin{align*}
    (\vec{r}'_{y_i,y_j})_m = (\mat{D} \vec{r}_{y_i,y_j})_{m} = (\vec{r}_{y_i,y_j})_{mN}.
\end{align*}
Thus $\vec{r}'_{y_i,y_j}$ is the $N$-decimation of $\vec{r}_{y_i,y_j}$. Let $\vec{y}'_i$ denote the $N$-decimation of $\vec{y}_i$. Then $\vec{y}'_i$ corresponds to the output of coset $i$. Now let $\vec{r}_{y'_i,y'_j} = \vec{y}'_i \circ \vec{y}'_j$.

\begin{blockTheorem} \lab{th:deci-corr}
    Let $Y_i[n]$ and $Y_j[n]$ be wide sense stationary stochastic processes such that $(\vec{y}_i)_m = Y_i[m]$ and $(\vec{y}_j)_m = Y_j[m]$ for $m=-LN+1,\ldots,LN-1$. Further let $Y'_i[n]$ and $Y'_j[n]$ be wide sense stationary stochastic processes such that $(\vec{y}'_i)_m = Y'_i[m]$ and $(\vec{y}'_j)_m = Y'_j[m]$ for $m=-L+1,\ldots,L-1$. Then $N\vec{r}_{y'_i,y'_j}$ is an unbiased estimator of $E( \vec{r}'_{y_i,y_j})$.
\end{blockTheorem}

\Cref{th:deci-corr} shows that the correlation of the outputs of cosets $i$ and $j$ can be used to estimate the $N$-decimation of the correlation of the pseudo outputs of cosets $i$ and $j$. To this end, let
\begin{align*}
    \vec{r}'_y = N \begin{bmatrix}
        \vec{y}'_1 \circ \vec{y}'_1 \\
        \vdots \\
        \vec{y}'_M \circ \vec{y}'_M
    \end{bmatrix}.
\end{align*}
Then
\begin{align*}
    E(\vec{r}'_y) &= \begin{bmatrix}
        E(N\vec{r}_{y'_1,y'_1}) \\
        \vdots \\
        E(N\vec{r}_{y'_M,y'_M})
    \end{bmatrix}
    = E\left(\begin{bmatrix}
        \vec{r}'_{y_1,y_1} \\
        \vdots \\
        \vec{r}'_{y_M,y_M} \\
    \end{bmatrix}\right) = E(\mat{R} \vec{r}_x) = \mat{R} E(\vec{r}_x).
\end{align*}
So $\vec{r}_y'$ is an unbiased estimator of $\mat{R} E(\vec{r}_x)$, which we can use to determine $E(\vec{r}_x)$. Denote $\vec{x}_m = \vec{x}[(m-1)N+1,mN]$ for $m = 1,\ldots,L$. Thus $\vec{x}_m$ corresponds to the $m$'th interval of $N$ samples of $\vec{x}$. Finally, note that
\begin{align*}
    (\vec{y}'_i)_m = (\vec{y}_i)_{mN} = (\vec{c}_i \ast \vec{x})_{mN} = \vec{d}_i \cdot \vec{x}_m
\end{align*}
where $\vec{d}_i$ denotes $\vec{c}_i$ reversed. Therefore, the reverse of the of the sampling vector for a coset determines the output of the coset for every interval of $N$ samples of $\vec{x}$. Accordingly, let
\begin{align*}
    \vec{w}_m = \begin{bmatrix}
        (\vec{y}'_1)_m \\
        \vdots \\
        (\vec{y}'_M)_m
    \end{bmatrix} = \begin{bmatrix}
        \vec{d}_1 \cdot \vec{x}_m \\
        \vdots \\
        \vec{d}_M \cdot \vec{x}_m
    \end{bmatrix} = \begin{bmatrix}
        \vec{d}_1^T\\
        \vdots \\
        \vec{d}_M^T
    \end{bmatrix} \vec{x}_m.
\end{align*}
Thus $\vec{w}_m$ aggregates the output of all cosets in the $m$'th interval of $N$ samples of $\vec{x}$. This concludes the main analysis.

\section{Proofs of Main Analysis}
In this section the theorems of the main analysis will be proven.

\begin{blockProofTheorem}{\ref{th:conv-comm}}
    A change of index yields that
    \begin{align*}
        (\vec{x} \ast \vec{y})_i &= \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k=-\infty}^{\infty} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k'=-\infty}^{\infty} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= \sum_{k'=1}^{M} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= (\vec{y} \ast \vec{x})_i.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:corr-unbiased}}
    Without loss of generality, assume that $M \le N$. Suppose that $i \le M$, then
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}
        })_{M-i+k}] \\
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{M-i+1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}] \\
        &= E(X[1] \conj{Y}[M-i+1]) + \ldots + E(X[i] \conj{Y}[M])  \\
        &= i R_{X,Y}[i-M]
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. Now suppose that $M < i \le N$, then
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}] \\
        &= E(X[i-M+1] \conj{Y}[1]) + \ldots + E(X[i] \conj{Y}[M]) \\  
        &= M R_{X,Y}[i-M]
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. Finally suppose that $i > N$, then
    \begin{align*}
        E[(\vec{x} \circ \vec{y})_i] &= E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_N (\conj{\vec{y}})_{M-i+N}] \\
        &= E(X[i-M+1] \conj{Y}[1]) + \ldots + E(X[N] \conj{Y}[M-i+N]) \\
        &= (N+M-i) R_{X,Y}[i-M]
    \end{align*}
    since $X[n]$ and $Y[n]$ are wide sense stationary. So
    \begin{align*}
        E(\vec{x} \circ \vec{y}) &= \begin{bmatrix}
            E[(\vec{x} \circ \vec{y})_1] \\
            E[(\vec{x} \circ \vec{y})_2] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_{M-1}] \\
            E[(\vec{x} \circ \vec{y})_{M}] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_N] \\
            E[(\vec{x} \circ \vec{y})_{N+1}] \\
            \vdots \\
            E[(\vec{x} \circ \vec{y})_{N+M-2}] \\
            E[(\vec{x} \circ \vec{y})_{N+M-1}]
        \end{bmatrix} \\
        &= \begin{bmatrix}
            R_{X,Y}[1-M] \\
            2 R_{X,Y}[2-M] \\
            \vdots \\
            (M-1) R_{X,Y}[-1] \\
            M R_{X,Y}[0] \\
            \vdots \\
            M R_{X,Y}[N-M] \\
            (M-1) R_{X,Y}[N+1-M] \\
            \vdots \\
            2 R_{X,Y}[N-2] \\
            R_{X,Y}[N-1]
        \end{bmatrix} \\
        &= \begin{bmatrix}
            1 \\
            2 \\
            \vdots \\
            M-1 \\
            M \\
            \vdots \\
            M \\
            M-1 \\
            \vdots \\
            2 \\
            1
        \end{bmatrix} \odot \begin{bmatrix}
            R_{X,Y}[1-M] \\
            R_{X,Y}[2-M] \\
            \vdots \\
            R_{X,Y}[-1] \\
            R_{X,Y}[0] \\
            \vdots \\
            R_{X,Y}[N-M] \\
            R_{X,Y}[N+1-M] \\
            \vdots \\
            R_{X,Y}[N-2] \\
            R_{X,Y}[N-1]
        \end{bmatrix}.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:conv-corr}}
    Note that
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        &= [(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x})]_m \\
        &=\sum_{k''=1}^{LN+N-1}\sum_{k=1}^N (\vec{c}_i)_k (\vec{x})_{k''-k+1}\sum_{k'=1}^{N}(\conj{\vec{c}}_j)_{k'}(\conj{\vec{x}})_{(LN+N-1-m+k'')-k'+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{k'=\infty}^{\infty}\sum_{k''=-\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{k'}(\vec{x})_{k''-k+1}(\conj{\vec{x}})_{(LN+N-1-m+k'')-k'+1}.
    \end{align*}
    To further evaluate this expression, we introduce a change of variables. To this end, let $k' = N -l'' +k$ and $k'' = l' + k - 1$. This transformation is invertible, so
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        % I don't think this step is necessary
        % &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}(\vec{x})_{(l' + k - 1)-k+1}(\vec{x})_{[LN+N-1-m+(l' + k - 1)]-(N -l'' +k)+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}(\vec{x})_{l'}
        (\conj{\vec{x}})_{LN-(m-l'' + 1)+l'} \\
        &=\sum_{l''=1}^{2N-1}\sum_{k=1}^{N}(\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}\sum_{l'=1}^{LN}(\vec{x})_{l'}
        (\conj{\vec{x}})_{LN-(m-l'' + 1)+l'} \\
        &=[(\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x})]_m.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:deci-corr}}
    Note that
    \begin{align*}
        R_{Y'_i,Y'_j}[m]
        &= E(\conj{Y}'_i[k]Y'_j[k+m]) \\
        &= E[(\conj{\vec{y}}'_i)_{k}(\vec{y}'_j)_{k+m}] \\
        &= E[(\conj{\vec{y}}_i)_{kN}(\vec{y}_j)_{kN+mN}] \\
        &= E(\conj{Y}_i[kN]Y_j[kN+mN]) \\
        &= R_{Y_i,Y_j}[mN].
    \end{align*}
    Therefore
    \begin{align*}
        E(N\vec{r}_{y'_i,y'_j})
        &= N E(\vec{y}'_i \circ \vec{y}'_j) \\
        &= N\begin{bmatrix}
            1 \\
            \vdots \\
            L \\
            \vdots \\
            1
        \end{bmatrix} \odot \begin{bmatrix}
            R_{Y'_i,Y'_j}[1-L] \\
            \vdots \\
            R_{Y'_i,Y'_j}[0] \\
            \vdots \\
            R_{Y'_i,Y'_j}[L-1]
        \end{bmatrix} \\
        &= \begin{bmatrix}
            N \\
            \vdots \\
            NL \\
            \vdots \\
            N
        \end{bmatrix} \odot \begin{bmatrix}
            R_{Y_i,Y_j}[N(1-L)] \\
            \vdots \\
            R_{Y_i,Y_j}[0] \\
            \vdots \\
            R_{Y_i,Y_j}[N(L-1)]
        \end{bmatrix} \\
        &= E(\vec{r}'_{y_i,y_j}).
    \end{align*}
\end{blockProofTheorem}

\section{The Algorithm}
This section will discuss an algorithm to estimate $E(\vec{r}_x)$ given the correlation vectors of all cosets.

\subsection{Unicity of Solution}
rank, min sparse ruler

\subsection{Algorithm}
Proceed as follows:
\begin{enumerate}
    \item Construct $\mat{R}$.
    \item Determine $\vec{y}'_i$ for $i = 1,\ldots,M$.
    \item Construct $\vec{r}'_y$.
    \item Determine $E(\vec{r}_x)$ by $\mat{R}^\dagger\vec{r}'_y$.
\end{enumerate}

\subsection{Oversampling}
Tikzplaatje

\section{Exploring Further Possibilities}
\subsection{Sample-Wide Autocorrelation}
Is it possible to estimate $E(\vec{r}_x)$ by making use of $\vec{w}_m$. To this end,
\begin{align*}
    (\vec{w}_m \circ \vec{w}_m)_i &= \sum_{k=1}^M (\vec{w}_m)_k (\conj{\vec{w}}_m)_{M-i+k} \\
    &= \sum_{k=1}^M (\vec{d}_k^T \vec{x}_m)(\conj{\vec{d}}_{M-i+k}^T \conj{\vec{x}}_m) \\
    &= \sum_{k=1}^M (\vec{x}_m^T \vec{d}_k)(\conj{\vec{d}}_{M-i+k}^T \conj{\vec{x}}_m)
\end{align*}
where $\vec{d}_i = \vec{0}$ for $i < 1$ and $i > M$. Let
\begin{align*}
     \mat{A}_i = \sum_{k=1}^M  \vec{d}_k \conj{\vec{d}}_{M-i+k}^T
\end{align*}
for $i = 1,\ldots,2M-1$. Then
\begin{align*}
    (\vec{w}_m \circ \vec{w}_m)_i &= \sum_{k=1}^M \vec{x}_m^T (\vec{d}_k \conj{\vec{d}}_{M-i+k}^T) \conj{\vec{x}}_m \\
    &= \vec{x}_m^T\left(\sum_{k=1}^M  \vec{d}_k \conj{\vec{d}}_{M-i+k}^T\right) \conj{\vec{x}}_m \\
    &= \vec{x}_m^T \mat{A}_i \conj{\vec{x}}_m \\
    &= \sum_{k = 1}^N \sum_{l=1}^N (\vec{x}_m)_k (\conj{\vec{x}}_m)_l (\mat{A}_i)_{k,l} \\
    &= \sum_{k = 1}^N \sum_{l=1}^N (\vec{x})_{(m-1)N+k} (\conj{\vec{x}})_{(m-1)N+l} (\mat{A}_i)_{k,l}.
\end{align*}
Now let $\vec{\mathfrak{x}} \in \mathbb{C}^{(LN)^2}$ be such that $(\vec{\mathfrak{x}})_{(i-1)LN+j} = (\vec{x})_i (\conj{\vec{x}})_j$ for $i = 1,\ldots,LN$ and $j = 1,\ldots,LN$. Then
\begin{align*}
    (\vec{w}_m \circ \vec{w}_m)_i = \vec{p}_{m,i}^T \vec{\mathfrak{x}}
\end{align*}
where $\vec{p}_{m,i} \in \mathbb{C}^{(LN)^2}$ such that
\begin{align*}
    (\vec{p}_{m,i})_{[(m-1)N+k-1]LN+(m-1)N+l} = (\mat{A}_i)_{k,l}
\end{align*}
for $k = 1,\ldots,N$ and $l = 1,\ldots,N$. Now let $\vec{r}_w$ and $\mat{P}$ be such that
\begin{align*}
    \vec{r}_w = \begin{bmatrix}
        (\vec{w}_1 \circ \vec{w}_1)_1 \\
        \vdots \\
        (\vec{w}_L \circ \vec{w}_L)_{2M-1}
    \end{bmatrix} = \begin{bmatrix}
        \vec{p}_{1,1}^T \vec{\mathfrak{x}} \\
        \vdots \\
        \vec{p}_{L,2M-1}^T \vec{\mathfrak{x}}
    \end{bmatrix} = \begin{bmatrix}
        \vec{p}_{1,1}^T \\
        \vdots \\
        \vec{p}_{L,2M-1}^T
    \end{bmatrix} \vec{\mathfrak{x}} = \mat{P} \vec{\mathfrak{x}}.
\end{align*}
Similarly, we can relate
\begin{align*}
    (\vec{r}_x)_i &= \sum_{k=1}^{LN}(\vec{x})_k (\conj{\vec{x}})_{LN-i+k} \\
    &= \vec{b}_i^T \vec{\mathfrak{x}}
\end{align*}
where $\vec{b}_i \in \mathbb{C}^{(LN)^2}$ is such that
\begin{align*}
    (\vec{b}_i)_{(k-1)LN+LN-i+k} = 1
\end{align*}
for $k = 1,\ldots,LN$ such that $1 \le LN-i+k \le LN$. Now let $\mat{B}$ be such that
\begin{align*}
    \vec{r}_x = \begin{bmatrix}
        (\vec{r}_x)_1 \\
        \vdots \\
        (\vec{r}_x)_{2LN-1}
    \end{bmatrix} = \begin{bmatrix}
        \vec{b}_1^T \vec{\mathfrak{x}} \\
        \vdots \\
        \vec{b}_{2LN-1}^T \vec{\mathfrak{x}}
    \end{bmatrix} = \begin{bmatrix}
        \vec{b}_1^T \\
        \vdots \\
        \vec{b}_{2LN-1}^T
    \end{bmatrix} \vec{\mathfrak{x}} = \mat{B} \vec{\mathfrak{x}}.
\end{align*}
This yields the system
\begin{align*}
    \vec{r}_w &= \mat{P} \vec{\mathfrak{x}}, \\
    \vec{r}_x &= \mat{B} \vec{\mathfrak{x}}
\end{align*}
which can be used in a similar way to estimate $E(\vec{r}_x)$. However, since $\mat{A}_i$ is an $N \times N$ matrix, all elements in $\vec{p}_{m,i}$ relating to $(\vec{x})_k (\conj{\vec{x}})_l$ where $|k - l| \ge N$ will be zero. Therefore using $\vec{r}_w$ to estimate $E(\vec{r}_x)$ yields that the estimation is limited in support from $-N+1$ to $N-1$. This damages the resolution severely.

\subsection{Discussion of \cref{th:corr-unbiased}}
In this section we discuss \cref{th:corr-unbiased}. First, one could propose the weights
\begin{align*}
    \begin{bmatrix}
        1 & \cdots & \frac{1}{K-1} & \frac{1}{K} & \cdots & \frac{1}{K} & \frac{1}{K-1} & \cdots & 1
    \end{bmatrix}^T
\end{align*}
to estimate the cross-correlation of $X[n]$ and $Y[n]$ from $-M+1$ to $N-1$ without bias. However, \cite{percival1993univariate} shows that this usually increases the mean squared error.

Second, $E(\vec{r}_x)$ is usually estimated to estimate the power spectral density of $X[n]$. Therefore, we study the effect of the weights discussed in \cref{th:corr-unbiased} on the power spectral density. Let $r[n]$ denote a discrete signal such that $r[i] = (\vec{r}_x)_{i+LN}$ for $i = -LN+1,\ldots,LN-1$. Then denote
\begin{align*}
    E(r[i])=E[(\vec{r}_x)_{i + LN}]=R_X[i] W[i]
\end{align*}
where
\begin{align*}
    W[i] = \begin{cases}
        i+LN  & -LN+1 \le i \le 0, \\
        LN-i& 0 < i \le LN-1, \\
        0 & \text{elsewhere.}
    \end{cases}
\end{align*}

Thus by the Wiener-Khintchine theorem and the convolution theorem,
\begin{align*}
    \DTFT\{E(r[i])\} = \frac{1}{2 \pi}\DTFT\{R_X[i]\} \circledast \DTFT\{W[i]\} = \frac{1}{2 \pi}\mathcal{P}(\omega) \circledast \DTFT\{W[i]\}
\end{align*}
where $\mathcal{P}(\omega)$ denotes the power spectral density of $X[n]$. Note that
\renewcommand{\pushrightwidth}{10cm}
\begin{align*}
    \DTFT\{W[i]\} &= \sum_{k=-\infty}^{\infty}W[k]\exp(-j \omega k) \\
    &= \sum_{k=-LN+1}^{0}(k +LN)\exp(-j \omega k) + \sum_{k=1}^{LN-1}(LN-k) \exp(-j \omega k) \\
    &= \sum_{k=0}^{LN-1} \left[(LN-k) \exp(j \omega k) + (LN-k) \exp(-j \omega k) \right] - LN \\
    &= \sum_{k=0}^{LN-1} \left[\left(LN+j\frac{d}{d \omega}\right) \exp(j \omega k) \right.
    \\& \pushright{\left. + \left(LN-j \frac{d}{d \omega}\right) \exp(-j \omega k) \right] - LN} \\
    &= \left(LN+j\frac{d}{d \omega}\right) \sum_{k=0}^{LN-1}\exp(j \omega k)
    \\& \pushright{+ \left(LN-j \frac{d}{d \omega}\right) \sum_{k=0}^{LN-1} \exp(-j \omega k) - LN} \\
    &= \left(LN+j\frac{d}{d \omega}\right) \frac{1-\exp(j \omega LN)}{1-\exp(j \omega)}
    \\& \pushright{+ \left(LN-j \frac{d}{d \omega}\right) \frac{1-\exp(-j \omega LN)}{1-\exp(-j \omega)} - LN}.
\end{align*}
Further algebraic simplification yields that
\begin{align*}
    \DTFT\{W[i]\} = \frac{\cos(LN \omega) - 1}{\cos(\omega) - 1}.
\end{align*}
Since $\DTFT\{E(r[i])\}$ is the result of $\mathcal{P}(\omega)$ convolved by $\DTFT\{W[i]\}$, $\DTFT\{E(r[i])\}$ respresents $\mathcal{P}(\omega)$ with finite accuracy. The resolution of the obtained power spectral density will be determined by the first zero of $\DTFT\{W[i]\}$. Therefore the resolution is given by
\begin{align*}
    \Delta \omega = \frac{\pi}{2LN}.
\end{align*}

\chapter{Detection}

\section{Preliminaries}

% \begin{blockDefinition}[Delta vector]
% The $N$ dimensional delta vector is represented by a vector $\vec{\delta} \in \mathbb{C}^N$such that:

% \begin{align*}
%   (\vec{\delta})_i &= 1 &&\text{for $i = \lfloor \frac{N}{2} \rfloor$} \\ 
%   (\vec{\delta})_i &= 0 &&\text{elsewhere.}
% \end{align*}
% \end{blockDefinition}
% \begin{blockDefinition}[Element wise multiplication]
% The result of the element wise multiplication of two $N$ dimensional vectors $\vec{a}$ and $\vec{b}$, denoted by

% \begin{align*}
%   \vec{a} \diamond \vec{b}
% \end{align*}
%  is given by a $N$ dimension vector $\vec{c}$ with element $(\vec{c})_i = (\vec{a})_i (\vec{b})_i$. 
% \end{blockDefinition}

\begin{blockDefinition}[Likelihood function under Hypothesis]
Given a hypothesis $\mathcal{H}$ and a realisation $\mathbf{x}$ of a random variable $\mathbf{X}$. Then $f_{\mathbf{X} | \mathcal{H}}(\mathbf{x})$ denotes the likelihood function of $\mathbf{x}$ given $\mathcal{H}$.
\end{blockDefinition}

\begin{blockDefinition}[Neyman-Pearson Test]
Given a continous random vector $\vec{\mathfrak{x}}$, and two hypotheses $\mathcal{H}_0$ and $\mathcal{H}_1$, the Neyman-Pearson test rejects that $\mathbf{x}$ has been produced under $\mathcal{H}_0$ in favor of $\mathcal{H}_1$ when

\begin{align*}
    \Lambda (\mathbf{x}) &= \frac{f_{\vec{\mathfrak{x}} | \mathcal{H}_0} (\mathbf{x})}{f_{(\vec{\mathfrak{x}} | \mathcal{H}_1}(\mathbf{x})} < \eta 
\end{align*}

The decision threshold $\eta$ is chosen such that $P(\Lambda(\vec{\mathfrak{x}}) < \eta) = \alpha$.
\end{blockDefinition}

\section{Main analysis}

Let $\vec{x} \in \mathbb{C}^{N}$ denote the received signal. %The input of the algorithm consists of an estimate of $E(\vec{r}_x)$), which we will denote by $\vec{r}'_x$. 

The algorithm must decide between to hypotheses:

\begin{align}\label{eq:hypotheses}
  \mathcal{H}_0&: \vec{x} = \vec{n}\\
  \mathcal{H}_1&: \vec{x} = \vec{s} + \vec{n}
\end{align}

in which $\vec{n}$ denotes a noise signal and $\vec{s}$ denotes the signal
at the receiver. 

% Under the assumption that $\vec{n}$ represents AWGN, we have that $(\vec{n})_i \sim \mathcal{N}(0, \sigma_n^2)$. Furthermore, its autocorrelation, which will be denoted by $\vec{r}_n$, is equal to:

% \begin{align*}
%   \vec{r}_n = \vec{\delta}
% \end{align*}

\subsection{Energy Detection}
Energy detection is based on the \emph{Neyman-Pearson Test} to decide wether there is or isn't a signal present in the received signal $\vec{x}$. Instead of using the likelihood function in the test statistic, energy detection, resorts to the \emph{log-likelihood}. Therefore,  given
the signal $\vec{x}$

\begin{align*}
\Lambda(\vec{x}) &= \frac{\log (\prod_{i=1}^N \frac{1}{\sqrt{2 \pi \sigma_n^2}} \exp ( \frac{(\vec{x})_i^2}{2\sigma_n^2}))}{\log (\prod_{i=1}^N \frac{1}{\sqrt{2 \pi (\sigma_n^2+\sigma_s^2)}} \exp ( \frac{(\vec{x})_i^2}{2(\sigma_n^2 + \sigma_s^2)}))} \\
&= \frac{n}{2}(2\pi\sigma_s^2 ) +( -\frac{1}{2\sigma_n^2} + \frac{1}{2(\sigma_n^2 + \sigma^2_s)})(\vec{x} \cdot \vec{x}) \\
&= a + b  (\vec{x} \cdot \vec{x})
\end{align*}

Observing that the constants $a$ and $b$ do not depend on the signal itself, we can simplify the test statistic to:

\begin{align*}
\Lambda'(\vec{x}) &= (\vec{x} \cdot \vec{x})
\end{align*} 

As stated in \cite{axell2012spectrum}, a constant false alarm rate (CFAR)of the detector is a desirable property. Under $\mathcal{H}_0$, $ \frac{2\Lambda'(\vec{x})}{\sigma_n^2}$ follows a chi-square distribution with 2N degrees of freedom \cite{%http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6584537.
 }. Approximating this chi-square distribution by a Gaussian, using the Central Limit theorem, it can be found that
 the probability of false alarm is equal to

 \begin{align}
 P_{fa} = P(\Lambda'(\vec{x}) > \eta) = Q(\frac{\eta -\sigma_n^2}{\sqrt{\frac{2}{N}\sigma^2_n}}).\label{eq:p_fa}
 \end{align} \cite{
 %http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6061767
 }
 Observe the dependency on the noise variance $\sigma_n^2$ in  \cref{eq:p_fa}. That is, to determine $\eta$ for a certain false alarm rate,
 it is necessary that one knows (an estimate of) $\sigma_n^2$.

By dividing $\Lambda'$ by $N$, we create another test statistic $\Lambda'' = E[(x)_i^2] = (r_{(\vec{x})_0})$, where $r_{\vec{x}}$ denotes the autocorrelation of the received signal. We can create an analogue test statistic for samples in the spectral domain by nothing that

\begin{align}
    E[(x)_i^2]  = \int_{-\infty}^{\infty} S_x (f) df
\end{align}

where $S_x(f)$ denotes the power spectral density of the signal. 


\subsection{Estimation of the noise variance}
To determine the threshold for the test statistic, it is necessary that we can estimate the noise variance 

% http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6809311
% http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6061767
 
\end{document}
