%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../includes/preamble.tex}
\addbibresource{../../../../includes/bibliography.bib}

\begin{document}

\section{Implementational details}
\label{sec:reconstruction-implementation}
This section will discuss some details of the algorithm which should be considered when implementing the algorithm. We assume that multi-coset sampling is used.

\subsection{Requirements of the sampling signals}
\label{sub:reconstruction-ci}
This subsection discusses sufficient requirements on the sampling signal $c_i[n]$ which yield a correct estimation. Remember that the sampling signal determines the way a coset samples the input signal. The derivation of these restrictions can be found in \cref{sec:reconstruction-derivation}. The requirements are as follows:

\begin{enumerate}
    \item Every $c_i[n]=0$ for $n < 0$ and $n > N-1$.
    \item All $c_i[n]=1$ for a single $n=n_i$ and otherwise zero.
    \item The circular sparse ruler problem, which is stated below, must be satisfied.
\end{enumerate}

\begin{description}
    \item[Circular sparse ruler problem] Consider $n_i$ for every coset $i$ as discussed in the second requirement. The set consisting of $|n_i - n_j|$ and $N-|n_i-n_j|$ for all combinations of cosets $i$ and $j$ must make up $0,\ldots,N-1$.
\end{description}

The circular sparse ruler problem will be further discussed in \cref{cha:sampling_methods}.

\subsection{Estimation of the cross-correlations}
\label{sub:reconstruction-estimation}
In Step 4 of the algorithm we have to estimate $r_{y_i,y_j}[m]$. Suppose that $y_i[n]$ is known for $n = 0,\ldots,KL-1$. We assumed that $r_{y_i,y_j}[m]=0$ for $|m|>L$. It then remains to estimate $r_{y_i,y_j}[m]$ for $|m| \le L$. An estimator of $r_{y_i,y_j}[m]$ is given by
\begin{align} \label{eq:ryij-est}
    \hat{r}_{y_i,y_j}[m] = \frac{1}{KL-|m|}\sum_{k=l}^{u}y_i[k]\conj{y}_j[k+m]
\end{align}
where $l=-\max\{0,m\}$ and $u=KL-1-\min\{0,m\}$ \cite{hayes1996statistical}. Notice that $E(\hat{r}_{y_i,y_j}[m])=r_{y_i,y_j}[m]$, which means that $\hat{r}_{y_i,y_j}[m]$ is an unbiased estimator of $r_{y_i,y_j}[m]$.
% Similarly, we define
% \begin{align*}
%     \hat{\vec{r}}_{y_i,y_j} = \begin{bmatrix}
%         \hat{r}_{y_i,y_j}[L] & \cdots & \hat{r}_{y_i,y_j}[-L]
%     \end{bmatrix}^T.
% \end{align*}
% Then $E(\hat{\vec{r}}_{y_i,y_j})=\vec{r}_{y_i,y_j}$, which means that $\hat{\vec{r}}_{y_i,y_j}$ is an unbiased estimator of $\vec{r}_{y_i,y_j}$. Now consider \cref{eq:ry-R-rx}. Notice that
% \begin{align*}
%     E(\hat{\vec{r}}_{y_i,y_j}) = \vec{r}_{y_i,y_j} = \mat{R} \vec{r}_x.
% \end{align*}
% Thus $\hat{\vec{r}}_{y_i,y_j}$ can be used to estimate $\vec{r}_x$. Denote this estimation of $\vec{r}_x$ by $\hat{\vec{r}}_x$.

\subsection{Sparsity}
\label{sub:reconstruction-sparsity}
Operations on large matrices can be computationally expensive. If a matrix is sparse, then functions designed for sparse matrices can be used. These functions can provide significant speedups. In the discussion on the unicity of the estimation in \cref{sec:reconstruction-derivation}, we argued that every row of $\mat{R}$ has exactly one nonzero element. Remember that $\mat{R}$ describes the relationship between the autocorrelation of the input signal and the observed quantities. Since $\mat{R}$ has $2LN+1$ columns, the fraction of nonzero elements of $\mat{R}$ is given by
\begin{align*}
    \rho_{\text{nz}}=\frac{1}{2LN+1}.
\end{align*}
Since $LN$ determines the resolution of the estimated power spectral density of the input signal, the product $LN$ is usually chosen large. This means that $\mat{R}$ is usually a sparse matrix. This property will be exploited \cref{sec:sparse-matrices}.

\subsection{Efficient generation of the matrices}
\label{sub:reconstruction-generation}
In Step 2 of the algorithm, $\mat{R}$ has to be constructed. It turns out that $\mat{R}$ can be generated efficiently using commonly available functions. To generate $\mat{R}$, we make use of the functions $\operatorname{toeplitz}$ and $\operatorname{tril}$. The functions $\operatorname{toeplitz}$ and $\operatorname{tril}$ are often found in software used for digital signal processing. The operation of these functions is illustrated in \cref{sec:reconstruction-generation-algorithm}. The algorithm to construct $\mat{R}$ is as follows:

\begin{enumerate}[labelindent=0pt,labelwidth=\widthof{\ref{last-item2}},label=Step \arabic*:,itemindent=1em,leftmargin=!]
    \item Let \begin{align*}
                \vec{c} = \begin{bmatrix} c_{i,j}[N-1] \; \cdots \; c_{i,j}[-N+1] \end{bmatrix}
\end{align*} where \begin{align*}
                c_{i,j}[m] = \sum_{k=-\infty}^{\infty}c_i[k] \conj{c}_j[k+m].
            \end{align*}
    \item Append $\vec{c}$ with $2(L-1)N+1$ zeros.
    \item Calculate $\operatorname{tril}[\operatorname{toeplitz}(\vec{c})]$.
    \item Omit the first $N-1$ rows and the last $N-1$ columns, and keep rows $1,N,\ldots,2LN+1$. This yields $\mat{R}_{i,j}$.
    \item Construct \begin{align*}
        \mat{R} = \begin{bmatrix}
            \mat{R}_{1,1} \\ \vdots \\ \mat{R}_{M,M}
        \end{bmatrix}.
    \end{align*}
    \label{last-item2}
\end{enumerate}

An example of the algorithm which illustrates its correctness is given in \cref{sec:reconstruction-generation-algorithm}. This algorithm can take advantage of highly optimised implementations of $\operatorname{toeplitz}$ and $\operatorname{tril}$. Furthermore, Step 4 can be vectorised or optimised memory-wise. Therefore, the algorithm allows for fast reconstruction of $\mat{R}$.
\end{document}