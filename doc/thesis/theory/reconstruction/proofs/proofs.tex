%!TEX program = xelatex
%!TEX root = ../../theory.tex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../includes/preamble.tex}
\addbibresource{../../../../includes/bibliography.bib}

\begin{document}


\section{Proofs}
\label{sec:proofs}
This section will discuss the proof of the theorems used in the main analysis.

\begin{blockProofTheorem}{\ref{th:conv-comm}}
    A change of index yields that
    \begin{align*}
        (\vec{x} \ast \vec{y})_i &= \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k=-\infty}^{\infty} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k'=-\infty}^{\infty} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= \sum_{k'=1}^{M} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= (\vec{y} \ast \vec{x})_i.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:correlation-bias}}
    Suppose that $1 \le i \le N$, then
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_i &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}})_{N-i+k}] \\
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{N-i+1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{N}] \\
        &= i R_{X,Y}[i-N] \\
        &= (N-|i-N|) R_{X,Y}[i-N]
    \end{align*}
    Therefore
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_{i+N} = (N-|i|) R_{X,Y}[i]
    \end{align*}
    for $-N + 1\le i \le 1$. Now suppose that $N < i \le 2N-1$, then
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_i &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}})_{N-i+k}] \\
        &= E[(\vec{x})_{i-N+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_{N} (\conj{\vec{y}})_{2N-i}] \\
        &= (2N-i) R_{X,Y}[i-N] \\
        &= (N-|i-N|) R_{X,Y}[i-N].
    \end{align*}
    Therefore
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_{i+N} = (N-|i|) R_{X,Y}[i]
    \end{align*}
    for $1 < i \le N-1$.
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:convolution-correlation}}
    Note that
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        &= [(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x})]_m \\
        &=\sum_{k''=1}^{KLN+N-1}\sum_{k=1}^N (\vec{c}_i)_k (\vec{x})_{k''-k+1}\sum_{k'=1}^{N}(\conj{\vec{c}}_j)_{k'}(\conj{\vec{x}})_{(KLN+N-1-m+k'')-k'+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{k'=\infty}^{\infty}\sum_{k''=-\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{k'}(\vec{x})_{k''-k+1}(\conj{\vec{x}})_{(KLN+N-1-m+k'')-k'+1}.
    \end{align*}
    To further evaluate this expression, we introduce a change of variables. To this end, let $k' = N -l'' +k$ and $k'' = l' + k - 1$. This transformation is invertible, so
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        % I don't think this step is necessary
        % &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}(\vec{x})_{(l' + k - 1)-k+1}(\vec{x})_{[KLN+N-1-m+(l' + k - 1)]-(N -l'' +k)+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}(\vec{x})_{l'}
        (\conj{\vec{x}})_{KLN-(m-l'' + 1)+l'} \\
        &=\sum_{l''=1}^{2N-1}\sum_{k=1}^{N}(\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}\sum_{l'=1}^{KLN}(\vec{x})_{l'}
        (\conj{\vec{x}})_{KLN-(m-l'' + 1)+l'} \\
        &=[(\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x})]_m.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:decimation-correlation}}
    Since $\vec{y}_i = \vec{c}_i \ast \vec{x}$ and $\vec{x}$ represents a wide sense stationary stochastic random process, $\vec{y}_i$ and therefore $\vec{y}'_i$ also represent wide sense stationary stochastic random processes. Therefore, there exist wide sense stationary stochastic random processes $Y_i[n]$ and $Y'_i[n]$ such that $(\vec{y}_i)_m=Y_i[m]$ for $m=1,\ldots,KLN+N-1$ and $(\vec{y}'_i)_m=Y'_i[m]$ for $m=1,\ldots,KL$. Then
    \begin{align*}
        R_{Y'_i,Y'_j}[m]
        &= E(\conj{Y}'_i[k]Y'_j[k+m]) \\
        &= E[(\conj{\vec{y}}'_i)_{k}(\vec{y}'_j)_{k+m}] \\
        &= E[(\conj{\vec{y}}_i)_{kN}(\vec{y}_j)_{kN+mN}] \\
        &= E(\conj{Y}_i[kN]Y_j[kN+mN]) \\
        &= R_{Y_i,Y_j}[mN].
    \end{align*}
    Therefore, by \cref{th:correlation-bias},
    \begin{align*}
        [E(N\hat{\vec{r}}_{y'_i,y'_j})]_{m+L}
        &= N[E(\vec{r}_{y'_i,y'_j})]_{m+L+(KL-L+1)-1} \\
        &= N[E(\vec{r}_{y'_i,y'_j})]_{m+KL} \\
        &= N (KL-|m|) R_{Y'_i,Y'_j}[N] \\
        &= (KLN -|mN|) R_{Y_i,Y_j}[mN].
    \end{align*}
    Furthermore, note that
    \begin{align*}
        [E(\hat{\vec{r}}_{y_i,y_j})]_{mN+LN-N+1} &= [E(\vec{r}_{y_i,y_j})]_{mN+LN-N+1 +(KLN-LN+2N-1) - 1} \\
        &=[E(\vec{r}_{y_i,y_j})]_{mN+KLN+N-1}.
    \end{align*}
    Since $X[i]=0$ for $KLN-N+1 \le i \le KLN$, $\vec{x}$ is zero for the last $N-1$ elements. Therefore $\vec{y}_i$ is zero for the last $N-1$ elements. Since these last $N-1$ elements are zero, they contribute do not to the elements expected value of $\vec{r}_{y_i,y_j}$. Therefore, similar to the proof of \cref{th:correlation-bias}, we obtain that
    \begin{align*}
        [E(\hat{\vec{r}}_{y_i,y_j})]_{mN+LN-N+1}
        &= [(KLN + N - 1) - (N - 1) - |mN|] R_{Y_i,Y_j}[mN] \\
        &= (KLN - |mN|) R_{Y_i,Y_j}.
    \end{align*}
    Finally,
    \begin{align*}
        [E(N\hat{\vec{r}}_{y'_i,y'_j})]_{m+L}
        &= [E(\hat{\vec{r}}_{y_i,y_j})]_{mN+LN-N+1} \\
        &= [E(\hat{\vec{r}}_{y_i,y_j})]_{(m+L-1)N+1} \\
        &= [E(\mat{D} \hat{\vec{r}}_{y_i,y_j})]_{m+L} \\
        &= [E(\hat{\vec{r}}'_{y_i,y_j})]_{m+L}.
    \end{align*}
\end{blockProofTheorem}

\end{document}