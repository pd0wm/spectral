%!TEX program = xelatex
%!TEX root = ../../theory.tex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../includes/preamble.tex}
\addbibresource{../../../../includes/bibliography.bib}

\begin{document}


\section{Proofs}
\label{sec:proofs}
This section will discuss the proof of the theorems used in the main analysis.

\begin{blockProofTheorem}{\ref{th:conv-comm}}
    A change of index yields that
    \begin{align*}
        (\vec{x} \ast \vec{y})_i &= \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k=-\infty}^{\infty} (\vec{x})_k (\vec{y})_{i-k+1} \\
        &= \sum_{k'=-\infty}^{\infty} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= \sum_{k'=1}^{M} (\vec{y})_{k'} (\vec{x})_{i-k'+1} \\
        &= (\vec{y} \ast \vec{x})_i.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:correlation-bias}}
    Without loss of generality, assume that $M \le N$. Suppose that $1 \le i \le M$, then
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_i &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}})_{M-i+k}] \\
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{M-i+1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}] \pushline
        \pushright{+~E[(\vec{x})_{i+1} (\conj{\vec{y}})_{M+1}] + \ldots + E[(\vec{x})_{N} (\conj{\vec{y}})_{M-i+N}]}
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{M-i+1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}] \\
        &= E(X[1] \conj{Y}[M-i+1]) + \ldots + E(X[i] \conj{Y}[M])  \\
        &= i R_{X,Y}[i-M].
    \end{align*}
    So $E(r[i])=(i+M)R_{X,Y}[i]$ for $-M + 1\le i \le -M+M$. Now suppose that $M < i < N$, then
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_i &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}})_{M-i+k}] \\
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{M-i+1}] + \ldots + E[(\vec{x})_{i-M} (\conj{\vec{y}})_{0}]+E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] \pushline
        \pushright{+~\ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}]+E[(\vec{x})_{i+1} (\conj{\vec{y}})_{M+1}] + \ldots + E[(\vec{x})_{N} (\conj{\vec{y}})_{M-i+N}]}
        &= E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_i (\conj{\vec{y}})_{M}] \\
        &= E(X[i-M+1] \conj{Y}[1]) + \ldots + E(X[i] \conj{Y}[M]) \\  
        &= M R_{X,Y}[i-M]
    \end{align*}
    So $E(r[i])=M R_{X,Y}[i]$ for $-M+M<i<N-M$. Finally suppose that $N \le i \le N +M - 1$, then
    \begin{align*}
        [E(\vec{x} \circ \vec{y})]_i &= \sum_{k=1}^N E[(\vec{x})_k (\conj{\vec{y}})_{M-i+k}] \\
        &= E[(\vec{x})_1 (\conj{\vec{y}})_{M-i+1})] + \ldots + E[(\vec{x})_{i-M} (\conj{\vec{y}})_{0}] \pushline
        \pushright{+~E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_N (\conj{\vec{y}})_{M-i+N}]}
        &= E[(\vec{x})_{i-M+1} (\conj{\vec{y}})_{1}] + \ldots + E[(\vec{x})_N (\conj{\vec{y}})_{M-i+N}] \\
        &= E(X[i-M+1] \conj{Y}[1]) + \ldots + E(X[N] \conj{Y}[M-i+N]) \\
        &= (N+M-i) R_{X,Y}[i-M]
    \end{align*}
    So $E(r[i])=(N-i)R_{X,Y}[i]$ for $N-M<i\le N-1$.
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:convolution-correlation}}
    Note that
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        &= [(\vec{c}_i \ast \vec{x}) \circ (\vec{c}_j \ast \vec{x})]_m \\
        &=\sum_{k''=1}^{KLN+N-1}\sum_{k=1}^N (\vec{c}_i)_k (\vec{x})_{k''-k+1}\sum_{k'=1}^{N}(\conj{\vec{c}}_j)_{k'}(\conj{\vec{x}})_{(KLN+N-1-m+k'')-k'+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{k'=\infty}^{\infty}\sum_{k''=-\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{k'}(\vec{x})_{k''-k+1}(\conj{\vec{x}})_{(KLN+N-1-m+k'')-k'+1}.
    \end{align*}
    To further evaluate this expression, we introduce a change of variables. To this end, let $k' = N -l'' +k$ and $k'' = l' + k - 1$. This transformation is invertible, so
    \begin{align*}
        (\vec{y}_i \circ \vec{y}_j)_m
        % I don't think this step is necessary
        % &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\vec{c}_j)_{N -l'' +k}(\vec{x})_{(l' + k - 1)-k+1}(\vec{x})_{[KLN+N-1-m+(l' + k - 1)]-(N -l'' +k)+1} \\
        &=\sum_{k=-\infty}^\infty\sum_{l''=-\infty}^{\infty}\sum_{l'=\infty}^{\infty} (\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}(\vec{x})_{l'}
        (\conj{\vec{x}})_{KLN-(m-l'' + 1)+l'} \\
        &=\sum_{l''=1}^{2N-1}\sum_{k=1}^{N}(\vec{c}_i)_k (\conj{\vec{c}}_j)_{N -l'' +k}\sum_{l'=1}^{KLN}(\vec{x})_{l'}
        (\conj{\vec{x}})_{KLN-(m-l'' + 1)+l'} \\
        &=[(\vec{c}_i \circ \vec{c}_j) \ast (\vec{x} \circ \vec{x})]_m.
    \end{align*}
\end{blockProofTheorem}

\begin{blockProofTheorem}{\ref{th:deci-corr}}
    Note that
    \begin{align*}
        R_{Y'_i,Y'_j}[m]
        &= E(\conj{Y}'_i[k]Y'_j[k+m]) \\
        &= E[(\conj{\vec{y}}'_i)_{k}(\vec{y}'_j)_{k+m}] \\
        &= E[(\conj{\vec{y}}_i)_{kN}(\vec{y}_j)_{kN+mN}] \\
        &= E(\conj{Y}_i[kN]Y_j[kN+mN]) \\
        &= R_{Y_i,Y_j}[mN].
    \end{align*}
    Therefore
    \begin{align*}
        [E(N\hat{\vec{r}}_{y'_i,y'_j})]_{m+L}
        &= [E(N\vec{r}_{y'_i,y'_j})]_{m+L+(KL-L+1)-1} \\
        &= [E(N\vec{r}_{y'_i,y'_j})]_{m+KL} \\
        &= N W_1[m] R_{Y'_i,Y'_j}[m] \\
        &= N W_1[m] R_{Y_i,Y_j}[mN]
    \end{align*}
    where
    \begin{align*}
        NW_1[m] &= \begin{cases}
            KLN+mN & \text{if } -KL+1 \le m \le 0, \\
            KLN-mN & \text{if } 0 \le m \le KL - 1, \\
            0 & \text{elsewhere}.
        \end{cases} \\
        % &= \begin{cases}
        %     mN+LN & \text{if } -LN+N \le mN \le 0, \\
        %     LN-mN & \text{if } 0 \le mN \le LN - N,\\
        %     0 & \text{elsewhere}.
        % \end{cases} \\
        % &= \begin{cases}
        %     mN+LN & \text{if } -LN+1 \le iN \le 0, \\
        %     LN-mN & \text{if } 0 \le iN \le LN - 1, \\
        %     0 & \text{elsewhere}.
        % \end{cases}
    \end{align*}
    Similarly note that
    \begin{align*}
        [E(\hat{\vec{r}}_{y_i,y_j})]_{mN+LN-N+1} &= [E(\vec{r}_{y_i,y_j})]_{mN+LN-N+1 +(KLN-LN+2N-1) - 1} \\
        &=[E(\vec{r}_{y_i,y_j})]_{mN+KLN+N-1}\\
        &= W_2[mN] R_{Y_i,Y_j}[mN]
    \end{align*}
    where
    \begin{align*}
        W_2[mN] &= \begin{cases}
            KLN+N-1+mN & \text{if } -KLN-N+2 \le mN \le 0, \\
            KLN+N-1-mN& \text{if } 0 \le mN \le KLN+N-2, \\
            0 & \text{elsewhere.}
        \end{cases} \\
        &= \begin{cases}
            KLN+N-1+mN & \text{if } -KL\le m \le 0, \\
            KLN+N-1-mN& \text{if } 0 \le m \le KL, \\
            0 & \text{elsewhere.}
        \end{cases}
    \end{align*}
    Let
    \begin{align*}
        W_{2/1}[m] &= \begin{cases}
            (KLN+N-1+mN)/(KLN+mN) & \text{if } -KL+1 \le m \le 0, \\
            (KLN+N-1-mN)/(KLN-mN) & \text{if } 0 \le m \le KL-1, \\
            0 & \text{elsewhere.}
        \end{cases}
    \end{align*}
    Then $W_{2/1}[m] N W_1[m] = W_2[mN]$ for $m = -KL+1,\ldots,KL-1$. To this end, let $\vec{w} \in \mathbb{C}^{2L-1}$ be such that $(\vec{w})_{i+L}=W_{1 \to 2}[i]$ for $i = -L+1,\ldots,L-1$. So
    \begin{align*}
        \vec{w} = \begin{bmatrix}
            KLN+(-L+2)N-1 \\
            KLN+(-L+3)N-1 \\
            \vdots \\
            KLN+0\cdot N-1 \\
            KLN+1\cdot N-1 \\
            KLN+0\cdot N-1 \\
            \vdots \\
            KLN+(-L+3)N-1 \\
            KLN+(-L+2)N-1
         \end{bmatrix} \odot \begin{bmatrix}
            [KLN+(-L+1)N]^{-1} \\
            [KLN+(-L+2)N]^{-1} \\
            \vdots \\
            (KLN -1 \cdot N)^{-1} \\
            (KLN+ 0 \cdot N)^{-1} \\
            (KLN -1 \cdot N)^{-1} \\
            \vdots \\
            [KLN+(-L+2)N]^{-1} \\
            [KLN+(-L+1)N]^{-1}
         \end{bmatrix}.
    \end{align*}
    We then recognise that
    \begin{align*}
        [E(\vec{w} \odot N\hat{\vec{r}}_{y'_i,y'_j})]_{m+L} &= [E(\hat{\vec{r}}_{y_i,y_j})]_{mN+LN-N+1} \\
        &= [E(\hat{\vec{r}}_{y_i,y_j})]_{(m+L-1)N+1} \\
        &= [E(\mat{D} \hat{\vec{r}}_{y_i,y_j})]_{m+L} \\
        &= [E(\hat{\vec{r}}'_{y_i,y_j})]_{m+L}.
        % (m+L+1)N-1 = mN + LN + N - 1
    \end{align*}
\end{blockProofTheorem}

\end{document}