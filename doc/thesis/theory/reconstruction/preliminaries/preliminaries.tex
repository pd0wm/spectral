%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../includes/preamble.tex}
\addbibresource{../../../../includes/bibliography.bib}

\begin{document}
\section{Preliminaries}

Vectors are denoted by lower-case bold-faced letters and matrices are denoted by upper-case bold-faced letters. Unless stated otherwise, a vector is always assumed to be a column vector. The complex conjugate of a vector $\vec{x}$ is denoted by $\bar{\vec{x}}$. The inner product of an inner product space is denoted by $\cdot$. We now introduce notation to denote elements of vectors and matrices.

\begin{blockDefinition}[Vector Element]
    Let $\vec{x} \in \mathbb{C}^N$. Then $(\vec{x})_i$ denotes the $i$'th element of $\vec{x}$ for $i = 1,\ldots,N$.
\end{blockDefinition}

\begin{blockDefinition}[Matrix Element]
    Let $\mat{A}$ be an $M \times N$ matrix. Then $(\mat{A})_{i,j}$ denotes the $j$'th element of the $i$'th row of $\mat{A}$ for $i = 1,\ldots,M$ and $j=1,\ldots,N$.
\end{blockDefinition}

\begin{blockDefinition}
    Let $\vec{x} \in \{1\}^N$. Then $\vec{1}_N$ denotes $\vec{x}$.
\end{blockDefinition}

We will also need notation to denote two uncommon vector operations.

\begin{blockDefinition}[Subvector]
    Let $\vec{x} \in \mathbb{C}^N$. Then $\vec{x}[a,b]$ denotes a vector $\vec{z} \in \mathbb{C}^{b-a+1}$ such that $(\vec{z})_i = (\vec{x})_{i+a-1}$ for $i = 1,\ldots,b-a+1$.
\end{blockDefinition}

\begin{blockDefinition}[Submatrix]
    Let $\mat{A}$ be an $M \times N$ matrix. Then $\mat{A}[a,b]$ denotes the $M \times b-a+1$ matrix $\mat{B}$ such that $(\mat{B})_{i,j}=(\mat{A})_{i,j+a-1}$ for $i = 1,\ldots,N$ and $j = 1,\ldots,b-a+1$.
\end{blockDefinition}

Note that subvectors and submatrices include both boundary elements.

\begin{blockDefinition}[Reverse of Vector]
    Let $\vec{x} \in \mathbb{C}^N$. Then the reverse of $\vec{x}$ denotes a vector $\vec{y} \in \mathbb{C}^N$ such that $(\vec{y})_i = (\vec{x})_{N-i+1}$ for $i = 1,\ldots,N$.
\end{blockDefinition}

We now define the convolution operator as a closed binary operation on vectors. The definition is similar to the definition of convolution for discrete signals. This implies that results similar to well-known theorems can be obtained.

\begin{blockDefinition}[Convolution]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \ast \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\vec{y})_{i-k+1}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{blockDefinition}

\begin{blockTheorem}[Commutativity of Convolution] \label{th:conv-comm}
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \ast \vec{y} = \vec{y} \ast \vec{x}$.
\end{blockTheorem}

The proof of \cref{th:conv-comm} will be given in \cref{sec:proofs}. We also define correlation as a closed binary operation on vectors. The definition is again similar to the definition of correlation for discrete signals.

\begin{blockDefinition}[Deterministic Cross-correlation]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$. Then $\vec{x} \circ \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^{N+M-1}$ such that
    \begin{align*}
        (\vec{z})_i = \sum_{k=1}^{N} (\vec{x})_k (\conj{\vec{y}})_{M-i+k}
    \end{align*}
    where $(\vec{x})_i=0$ for $i < 1$ and $i > N$ and $(\vec{y})_i=0$ for $i < 1$ and $i > M$.
\end{blockDefinition}

Furthermore, we define the Hadamard product as a closed binary operation on vector.

\begin{blockDefinition}[Hadamard Product]
    Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^N$. Then $\vec{x} \odot \vec{y}$ denotes a vector $\vec{z} \in \mathbb{C}^N$ such that $(\vec{z})_i = (\vec{x})_i (\vec{y})_i$ for $i = 1,\ldots,N$.
\end{blockDefinition}

Note that $\odot$ is similar to $\cdot$, since the inner product and Hadamard product are closely related.
The following theorem identifies the expected value of the correlation operator.

\begin{blockTheorem} \lab{th:correlation-bias}
    Let $X[n]$ and $Y[n]$ be wide sense stationary stochastic processes. Let $\vec{x} \in \mathbb{C}^N$ and $\vec{y} \in \mathbb{C}^M$ be such that $(\vec{x})_i = X[i]$ for $i=1,\ldots,N$ and $(\vec{y})_i = Y[i]$ for $i=1,\ldots,M$. Furthermore, let $r[n]$ be a discrete signal such that $r[i] = (\vec{x} \circ \vec{y})_{i+M}$ for $i = -M+1,\ldots,N-1$. Then $r[i]$ is an unbiased estimator of $W[i]R_{X,Y}[i]$ where
    \begin{align*}
        W[i] = \begin{cases}
            i + M & \text{if } -M + 1 \le i \le -M + K, \\
            K & \text{if } -M + K < i < N - K, \\
            N - i & \text{if } N - K \le i \le N - 1, \\
            0 & \text{elsewhere,}
        \end{cases}
    \end{align*}
    where $K = \min\{N,M\}$.
\end{blockTheorem}

Finally, we extend the use of dots to denote finite sequences.

\begin{blockDefinition} \lab{def:dots-extended}
    Let $N \in \mathbb{N}$. Then $(1,1),\ldots,(N,N)$ denotes the sequence

    \makebox[\textwidth]{\centering
        $(1,1),\ldots(1,N),(2,1),\ldots,(2,N),(3,1),\ldots,(N,N).$
    } \nolinebreak
\end{blockDefinition}
\end{document}