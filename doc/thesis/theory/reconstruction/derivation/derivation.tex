%!TEX program = xelatex

\documentclass[a4paper, openany, oneside]{memoir}
\input{../../../../includes/preamble.tex}
\addbibresource{../../../../includes/bibliography.bib}

\begin{document}

\section{Derivation}
Consider the input signal to be a random stochastic process. Let the input signal be denoted by $x[n]$. Let the number of cosets by given by $M$. We denote the output of a coset $i$ by $y_i[n]$. To relate the output of a coset to the input signal, we need some mathematical constructs.

\subsection{Construction of the output of a coset}
Every coset $i$ is associated to a signal $c_i[n]$ such that $c_i[n] = 0$ for $n < 0$ and $n \ge N$. Then let
\begin{align*}
    z_i[n] = (c_i \ast x)[n]
\end{align*}
where $\ast$ denotes the convolution operator. The definition of the convolution operator yields that
\begin{align*}
    z_i[n] &= \sum_{k=\infty}^{\infty}x[k] c_i[nN-k] \\
    &= \sum_{k=n-N+1}^{n} x[k] c_i[n-k],
\end{align*}
since $c_i[n-k]=0$ for $k \le n-N$ and $k > n$. \cref{fig:visualisation-zi} shows how $z_i[n]$, $x[n]$ and $c_i[n]$ are related in the case that $N=4$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % \newcommand{\drawitem}[3]{
        %     \begin{scope}[shift={(0,#1*-3)}]
        \foreach \n in {0,1,2} {
            \begin{scope}[shift={(0,-2.5*\n)}]
                \draw [black] (-5,0.5) -- (5,0.5);
                \draw [black] (-5,0) -- (5,0);

                \draw [black] (-4+\n,-1) -- (0+\n,-1);
                \draw [black] (-4+\n,-0.5) -- (0+\n,-0.5);
                \foreach \i in {-5,...,5} {
                    \draw [black] (\i, 0) -- (\i, 0.5);
                }
                \foreach \i in {-4,...,5} {
                    \draw ({\i-0.5},0.25) node[black] {$x[\i]$};
                }

                \foreach \i in {-4,...,0} {
                    \draw [black] (\i+\n, -0.5) -- (\i+\n, -1);
                }
                \foreach \i in {0,...,3} {
                    \draw ({(3-\i)-3.5+\n},-0.75) node[black] {$c_i[\i]$};
                }

                \foreach \i in {-4,...,-1} {
                    \draw (\i+0.5+\n,-0.25) node {$\times$};
                }

                \draw [black, >=latex, ->] (-4.05+\n,-0.75) -- node[pos=0.45,fill=white] {\tiny$\sum$} (-5.95,-0.75);
                \draw [black] (-7,-0.5) -- (-7,-1) -- (-6,-1) -- (-6,-0.5) -- (-7,-0.5);
                \draw (-6.5,-0.75) node[black] {$z_i[\n]$};
                \draw (-5.5,0.25) node {$\cdots$};
                \draw (5.5,0.25) node {$\cdots$};
            \end{scope}
        }
    \end{tikzpicture}
    \caption{Relationship between $z_i[n]$, $x[n]$ and $c_i[n]$ in the case that $N=4$}
    \label{fig:visualisation-zi}
\end{figure}
We now relate the output of a coset to the input signal by
\begin{align*}
    y_i[n]=z_i[nN].
\end{align*}
Using the expression for $z_i[n]$, we obtain that 
\begin{align*}
    y_i[n]=\sum_{k=(n-1)N+1}^{nN} x[k]c_i[nN-k].
\end{align*}
\cref{fig:visualisation-yi} shows how $y_i[n]$ is constructed in the case that $N=4$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw [black] (-5,0.5) -- (5,0.5);
        \draw [black] (-5,0) -- (5,0);

        \draw [black] (-4,-1) -- (0,-1);
        \draw [black] (-4,-0.5) -- (0,-0.5);
        \draw [black] (0,-1.5) -- (4,-1.5);
        \draw [black] (0,-1) -- (4,-1);
        \foreach \i in {-5,...,5} {
            \draw [black] (\i, 0) -- (\i, 0.5);
        }
        \foreach \i in {-4,...,5} {
            \draw ({\i-0.5},0.25) node[black] {$x[\i]$};
        }

        \foreach \i in {-4,...,0} {
            \draw [black] (\i, -0.5) -- (\i, -1);
        }
        \foreach \i in {0,...,3} {
            \draw ({(3-\i)-3.5},-0.75) node[black] {$c_i[\i]$};
        }

        \foreach \i in {0,...,4} {
            \draw [black] (\i, -1.5) -- (\i, -1);
        }
        \foreach \i in {0,...,3} {
            \draw ({(3-\i)+0.5},-1.25) node[black] {$c_i[\i]$};
        }

        \foreach \i in {-4,...,-1} {
            \draw (\i+0.5,-0.25) node {$\times$};
        }
        \foreach \i in {0,...,3} {
            \draw [black, >=latex, ->] (\i+0.5,-0.05)  -- node[pos=0.45,fill=white] {$\times$} (\i+0.5,-0.95);
        }

        \draw [black, >=latex, ->] (-4.05,-0.75) -- node[pos=0.45,fill=white] {\tiny$\sum$} (-5.95,-0.75);
        \draw [black, >=latex, ->] (-0.05,-1.25) -- node[pos=0.825,fill=white] {\tiny$\sum$} (-5.95,-1.25);
        \draw [black] (-7,-0.5) -- (-7,-1) -- (-6,-1) -- (-6,-0.5) -- (-7,-0.5);
        \draw [black] (-7,-1) -- (-7,-1.5) -- (-6,-1.5) -- (-6,-1) -- (-7,-1);
        \draw (-6.5,-0.75) node[black] {$y_i[0]$};
        \draw (-6.5,-1.25) node[black] {$y_i[1]$};
        \draw (-5.5,0.25) node {$\cdots$};
        \draw (5.5,0.25) node {$\cdots$};
        \draw (-6.5,-1.75) node {$\vdots$};
        \draw (-6.5,0) node {$\vdots$};
    \end{tikzpicture}
    \caption{Relationship between $y_i[n]$, $x[n]$ and $c_i[n]$ in the case that $N=4$}
    \label{fig:visualisation-yi}
\end{figure}
BETERE UTILEG:
Consider the case that $c_i[n]=1$ for a single $n$ and otherwise zero. Then $y_i[n]$ consists of the same sample of every group of $N$ samples of $x[n]$. Thus $y_i[n]$ is an $N$-decimation of the input signal. \cref{fig:visualisation-yi-example} illustrates this concept. This summarises the relationship between $y_i[n]$, $x[n]$ and $c_i[n]$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw [black] (-5,0.5) -- (5,0.5);
        \draw [black] (-5,0) -- (5,0);

        \draw [black] (-4,-1) -- (0,-1);
        \draw [black] (-4,-0.5) -- (0,-0.5);
        \draw [black] (0,-1.5) -- (4,-1.5);
        \draw [black] (0,-1) -- (4,-1);
        \foreach \i in {-5,...,5} {
            \draw [black] (\i, 0) -- (\i, 0.5);
        }
        \foreach \i in {-4,...,5} {
            \draw ({\i-0.5},0.25) node[black] {$x[\i]$};
        }

        \foreach \i in {-4,...,0} {
            \draw [black] (\i, -0.5) -- (\i, -1);
        }
        \draw (-3.5, -0.75) node[black] {$0$};
        \draw (-2.5, -0.75) node[black] {$0$};
        \draw (-1.5, -0.75) node[black] {$1$};
        \draw (-0.5, -0.75) node[black] {$0$};

        \foreach \i in {0,...,4} {
            \draw [black] (\i, -1.5) -- (\i, -1);
        }
        \draw (0.5, -1.25) node[black] {$0$};
        \draw (1.5, -1.25) node[black] {$0$};
        \draw (2.5, -1.25) node[black] {$1$};
        \draw (3.5, -1.25) node[black] {$0$};


        \foreach \i in {-4,...,-1} {
            \draw (\i+0.5,-0.25) node {$\times$};
        }
        \foreach \i in {0,...,3} {
            \draw [black, >=latex, ->] (\i+0.5,-0.05)  -- node[pos=0.45,fill=white] {$\times$} (\i+0.5,-0.95);
        }

        \draw [black, >=latex, ->] (-4.05,-0.75) -- node[pos=0.45,fill=white] {\tiny$\sum$} (-5.95,-0.75);
        \draw [black, >=latex, ->] (-0.05,-1.25) -- node[pos=0.825,fill=white] {\tiny$\sum$} (-5.95,-1.25);
        \draw [black] (-7,-0.5) -- (-7,-1) -- (-6,-1) -- (-6,-0.5) -- (-7,-0.5);
        \draw [black] (-7,-1) -- (-7,-1.5) -- (-6,-1.5) -- (-6,-1) -- (-7,-1);
        \draw (-6.5,-0.75) node[black] {$x[-1]$};
        \draw (-6.5,-1.25) node[black] {$x[3]$};
        \draw (-5.5,0.25) node {$\cdots$};
        \draw (5.5,0.25) node {$\cdots$};
        \draw (-6.5,-1.75) node {$\vdots$};
        \draw (-6.5,0) node {$\vdots$};
    \end{tikzpicture}
    \caption{Relationship between $y_i[n]$, $x[n]$ and $c_i[n]$ in the case that $N=4$ and $c_i[n]=1$ for $n=1$ and otherwise zero}
    \label{fig:visualisation-yi-example}
\end{figure}

\subsection{Autocorrelation and cross-correlation}
The goal of the reconstruction method is to reconstruct the autocorrelation of the input signal given the output of all cosets. The autocorrelation of the input signal is given by
\begin{align*}
    r_x[n,m] = E(x[n]\conj{x}[n+m]).
\end{align*}
We assume that $x[n]$ is a wide sense stationary process. This means that $r_x[n,m]$ is independent of $n$. We therefore omit the $n$ in $r_x[n,m]$ and denote $r_x[n,m]=r_x[m]$. The cross-correlation of $z_i[n]$ and $z_j[n]$ is given by
\begin{align*}
    r_{z_i,z_j}[n,m]=E(z_i[n]\conj{z}_j[n+m]).
\end{align*}
Similarly, the cross-correlation of $y_i[n]$ and $y_j[n]$ is given by
\begin{align*}
    r_{y_i,y_j}[n,m]=E(y_i[n]\conj{y}_j[n+m]).
\end{align*}

\subsection{Relating the autocorrelation of the input signal}

Since $x[n]$ is a wide sense stationary process, Theorem 11.5 of stoch boek yields that
\begin{align*}
    r_{z_i,z_j}[n,m] = (c_{i,j} \ast r_{x})[m]
\end{align*}
where
\begin{align*}
    c_{i,j}[m] = \sum_{k=-\infty}^{\infty}c_i[k] \conj{c}_j[k+m].
\end{align*}
This shows that $r_{z_i,z_j}[n,m]$ is also independent of $n$. The independence implies that $r_{z_i,z_j}[n,m]$ is the same for every $n$, so
\begin{align*}
    r_{z_i,z_j}[mN] &= E(z_i[n]\conj{z}_j[n+mN]) \\
    &= E(z_i[nN]\conj{z}_j[nN+mN]) \\
    &= E(y_i[n]\conj{y}_j[n+m]) \\
    &= r_{y_i,y_j}[m],
\end{align*}
since we defined that $z_i[n]=y_i[nN]$. Therefore
\begin{align*}
    r_{y_i,y_j}[m] &= (c_{i,j}\ast r_{x})[mN].
\end{align*}
We know that $c_i[n]=0$ for $n < 0$ and $n \ge N$. The definition of $c_{i,j}[m]$ then yields that $c_{i,j}[m]=0$ for $|m| \ge N$, since $c_i[k]\conj{c}_j[k+m]$ is when $|m| \ge N$. So
\begin{align} \label{eq:ryiyj-rx}
    r_{y_i,y_j}[m] &= \sum_{k=-\infty}^{\infty}r_{x}[k]c_{i,j}[mN-k] \nonumber \\
    &= \sum_{k=(m-1)N+1}^{(m+1)N-1}r_{x}[k]c_{i,j}[mN-k].
\end{align}
This is the desired equation which relates the cross-correlation of the outputs of cosets $i$ and $j$ to the autocorrelation of the input signal. We see that every element of $r_{y_i,y_j}[m]$ depends on specific elements of $r_x[m]$. Consider the case that $N=3$. Then $c_{i,j}[m]=0$ for $|m| \ge 3$. \cref{fig:visualisation-ryiyj-rx} now illustrates the dependencies introduced by \cref{eq:ryiyj-rx}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \foreach \i in {-8,...,8} {
            \draw (-4.1,\i/2) -- (4, \i/2);
            \draw (\i/2, -4.1) -- (\i/2, 4);
            \draw (\i/2, -4.4) node {\small \num{\i}};
            \draw (-4.4, \i/2) node {\small \num{\i}};
        }

        \foreach \i in {-6,...,6} {
            \draw (\i/2, -4.4) node[red] {\small \num{\i}};
        }
        \foreach \i in {-2,...,2} {
            \draw (\i/2, -4.4) node[blue] {\small \num{\i}};
        }
        \foreach \i in {-4,...,4} {
            \draw (-4.4, \i/2) node[blue] {\small \num{\i}};
        }

        \draw [blue, thick, line width=1.5pt] (-4.1, -2) -- (-3, -2) -- (-3, -4.1);
        \draw [red, thick, line width=1.5pt] (-4.1, -2.5) -- (-1.5, -2.5) -- (-1.5, -4.1);
        \draw [blue, thick, line width=1.5pt] (-4.1, 2) -- (3, 2) -- (3, -4.1);
        \draw [red, thick, line width=1.5pt] (-4.1, 2.5) -- (1.5, 2.5) -- (1.5, -4.1);


        \foreach \i in {-2,...,2} {
            \foreach \j in {-8,...,8} {
                \draw [fill=black] (\j/2, {max(min(\i/2+\j/2, 4), -4)}) circle[radius=0.075,black];
            }
        }
        \draw (-5.4,0) node {$r_{y_i,y_j}[m]$};
        \draw (0,-5) node {$r_x[m]$};
    \end{tikzpicture}
    \caption{Illustration of the dependencies between the elements $r_{y_i,y_j}[m]$ and the elements of $r_x[m]$ introduced by \cref{eq:ryiyj-rx} in the case that $N=3$. The numbers at the axes represent the elements of the associated signal. A dot at the intersection of two lines represents a dependency between the associated elements.}
    \label{fig:visualisation-ryiyj-rx}
\end{figure}


\subsection{Estimating the autocorrelation of the input signal}
 Suppose that $N=3$. Also suppose that $r_{y_i,y_j}[m]$ is known for $|m| \le 4$. Furthermore, assume that $r_{y_i,y_j}[m]=0$ for $|m| \ge 5$. Since \cref{eq:ryiyj-rx} relates $r_{y_i,y_j}[m]$ to $r_x[m]$, we can use the known elements of $r_{y_i,y_j}[m]$ to solve for $r_x[m]$. In this example we investigate which elements of $r_x[m]$ can be solved for.

Again consider \cref{fig:visualisation-ryiyj-rx}. The known elements of $r_{y_i,y_j}[m]$ are depicted in blue. If we follow the blue lines, we can find out which elements of $r_x[m]$ are related to the known elements of $r_{y_i,y_j}[m]$. Therefore, the picture shows that given $r_{y_i,y_j}[m]$ for $|m| \le 4$, we should be able to solve $r_x[m]$ for $|m| \le 6$. However, we have to be careful, since we assumed that $r_{y_i,y_j}[m]=0$ for $|m| \ge 5$. If we follow the red lines, we find out that $r_{y_i,y_j}[m]=0$ for $|m| \ge 5$ implies that $r_x[m]$ can only be non-zero for $|m| \le 2$. Therefore, although we related $r_x[m]$ for $|m| \le 6$, we can only solve $r_x[m]$ for $|m| \le 2$.

We can look at this problem in a more general way. Assume that $r_{y_i,y_j}[m]=0$ for $|m| > L$. Then observe that
\begin{align*}
    r_{y_i,y_j}[L+1] &= \sum_{k=LN+1}^{(L+1)N-1} r_x[k] c_{i,j}[k-(L+1)N].
\end{align*}
Since $c_{i,j}[m]$ is arbitrary, $r_{y_i,y_j}[L+1]=0$ implies that $r_x[m]=0$ for $LN+1 \le m \le (L+1)N-1$. Therefore, if we evaluate $r_{y_i,y_j}[m]=0$ for all $|m| > L$ in a similar way, then $r_x[m]=0$ for $|m| > LN$.

Since $r_x[m]=0$ for $|m| > LN$, all that remains is estimating $r_x[m]$ for $|m| \le LN$. Aggregating \cref{eq:ryiyj-rx} for $|m| \le L$ yields that
\begin{align*}
    r_{y_i,y_j}[L] &= \sum_{k=(L-1)N+1}^{(L+1)N-1}r_{x}[k]c_{i,j}[LN-k] = \sum_{k=(L-1)N+1}^{LN}r_{x}[k]c_{i,j}[LN-k], \\
    r_{y_i,y_j}[L-1] &= \sum_{k=(L-2)N+1}^{LN-1}r_{x}[k]c_{i,j}[(L-1)N-k], \\
    &~~\vdots \\
    % r_{y_i,y_j}[0] &= \sum_{k=1-N}^{N-1}r_{x}[k]c_{i,j}[-k], \\
    % &~~\vdots \\
    r_{y_i,y_j}[-L+1] &= \sum_{k=-LN+1}^{(-L+2)N-1}r_{x}[k]c_{i,j}[(-L+1)N-k], \\
    r_{y_i,y_j}[-L] &= \sum_{k=(-L-1)N+1}^{(-L+1)N-1}r_{x}[k]c_{i,j}[-LN-k] = \sum_{k=-LN}^{(-L+1)N-1}r_{x}[k]c_{i,j}[-LN-k]. \\
\end{align*}
Although these equations look complicated, they can be written more compactly. To do this, let
\begin{align*}
    \vec{r}_x[k] =& \begin{bmatrix}
        r_x[(k+1)N-1] & \cdots & r_x[kN+1]
    \end{bmatrix}^T, \\
    \vec{c}^{-}_{i,j} =& \begin{bmatrix}
        c_{i,j}[1-N] & \cdots & c_{i,j}[-1]
    \end{bmatrix} \text{ and } \\
    \vec{c}^{+}_{i,j} =& \begin{bmatrix}
        c_{i,j}[1] & \cdots & c_{i,j}[N-1]
    \end{bmatrix}.
\end{align*}
The complicated system of equations now reduces to
\begin{align*}
    r_{y_i,y_j}[L] &= &&\hspace{12pt}r_x[LN] c_{i,j}[0] &&+ \vec{c}_{i,j}^+ \vec{r}_x[L-1] , \\
    r_{y_i,y_j}[L-1] &= \vec{c}_{i,j}^{-} \vec{r}_x[L-1] &&+ r_x[(L-1)N] c_{i,j}[0] &&+ \vec{c}_{i,j}^{+} \vec{r}_x[L-2], \\
    &~~\vdots \\
    r_{y_i,y_j}[-L+1] &= \vec{c}_{i,j}^{-}\vec{r}_x[-L+1] &&+ r_x[(-L+1)N] c_{i,j}[0] &&+ \vec{c}_{i,j}^{+}\vec{r}_x[-L], \\
    r_{y_i,y_j}[-L] &= \vec{c}_{i,j}^{-} \vec{r}_x[-L] &&+ r_x[-LN] c_{i,j}[0],
\end{align*}
which can be conveniently written as
\begin{align*}
    \begin{bmatrix}
        r_{y_i,y_j}[L] \\
        r_{y_i,y_j}[L-1] \\
        \vdots \\
        r_{y_i,y_j}[-L+1] \\
        r_{y_i,y_j}[-L]
    \end{bmatrix} = \begin{bmatrix}
        c_{i,j}[0] & \vec{c}^+_{i,j} \\
        \vec{c}^{-}_{i,j} & c_{i,j}[0] & \vec{c}^{+}_{i,j} \\
        % &\vec{c}^{-}_{i,j} & c_{i,j}[0] & \vec{c}^{+}_{i,j} \\
        &&\ddots \\
        &&\vec{c}^{-}_{i,j} & c_{i,j}[0] & \vec{c}^{+}_{i,j} \\
        &&&\vec{c}^{-}_{i,j} & c_{i,j}[0]
    \end{bmatrix} \begin{bmatrix}
        r_x[LN] \\
        r_x[LN-1] \\
        \vdots \\
        r_x[-LN+1] \\
        r_x[-LN]
    \end{bmatrix}.
\end{align*}
Denote this equation by $\vec{r}_{y_i,y_j} = \mat{R}_{i,j} \vec{r}_x$.
\end{document}